{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SuperSpike - Offline.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVe0MtsWU3v8PA53jaZKyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyanshgupt/Unreliable-Transmission/blob/main/SuperSpike_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOHbh13pMBpS"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ru0F0ywEE_f"
      },
      "source": [
        "#@title Dependencies\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyiUtSYFJwPw"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Uncomment the line below to run on GPU\n",
        "#device = torch.device(\"cuda:0\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW1dH6WzKDXk"
      },
      "source": [
        "### Surrogate Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZzyUZYxKC1E"
      },
      "source": [
        "#@title Surrogate Gradient\n",
        "\n",
        "class SurrGradSpike(torch.autograd.Function):\n",
        "\n",
        "  scale = 100.0 # controls the steepness of the gradient\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    '''\n",
        "    computes a step-function on the input. ctx is a context variable\n",
        "    that stores information needed later for backpropagation\n",
        "    '''\n",
        "    ctx.save_for_backward(input)\n",
        "    out = torch.zeros_like(input)\n",
        "    out[input > 0] = 1\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    '''\n",
        "    In the backward method, we recieve a tensor we need to compute \n",
        "    the surrogradient of the loss with respect to the input. \n",
        "    Here we use the negative half of the fast sigmoid as in \n",
        "    Zenke & Ganguli 2018.\n",
        "    \n",
        "    '''\n",
        "    input, _ = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "    return grad\n",
        "\n",
        "# overwrite the spike function with the surrograte gradient function\n",
        "# using the apply method\n",
        "spike_fn = SurrGradSpike.apply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3TRO2gVEuTt"
      },
      "source": [
        "#@title Poisson Train Generator\n",
        "def Poisson_trains(n, lam, timesteps, dt):\n",
        "  \"\"\"\n",
        "\n",
        "  inputs:\n",
        "    n - number of poisson spike trains \n",
        "    lam - 1-D array containing mean value of poisson trains\n",
        "  Returns\n",
        "\n",
        "  \"\"\"\n",
        "  trains = torch.zeros((n, timesteps), device=device, dtype=dtype, requires_grad=True)\n",
        "  unif = torch.rand((n, timesteps), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "#  counter = 0\n",
        "  for i in range(n):\n",
        "    trains[unif <= lam[i]*dt] = 1\n",
        "#    counter += len(unif <= lam[i]*dt)\n",
        "#  print(\"Total No. of Spikes\", counter)\n",
        "\n",
        "  return trains"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_rqeRdPJ3I0"
      },
      "source": [
        "### Single Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLlECsCaJ0Bw"
      },
      "source": [
        "nb_inputs  = 100\n",
        "nb_outputs = 1\n",
        "\n",
        "nb_steps = 5000\n",
        "timestep_size = 1e-4 # 0.1 msec timesteps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWjXC58iKZSP",
        "outputId": "6c1ed86c-11b4-4991-8398-4d2fdd5fca6e"
      },
      "source": [
        "#@title LIF Neuron Model Parameters\n",
        "args = {'thres': -50,\n",
        "        'U_rest': -60,\n",
        "        'tau_mem': 1e-2,\n",
        "        'tau_syn': 5e-3,\n",
        "        'tau_ref': 5e-3,\n",
        "        't_rise': 5e-3, # the pre-synaptic double exponential kernel rise time\n",
        "        't_decay': 1e-2, # the pre-synaptic double exponential kernel decay time\n",
        "        'timestep_size': 1e-4} \n",
        "\n",
        "tau_syn = args['tau_syn']\n",
        "tau_mem = args['tau_mem']\n",
        "\n",
        "alpha = float(np.exp(-timestep_size/tau_syn))\n",
        "beta = float(np.exp(-timestep_size/tau_mem))\n",
        "print(beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9900498337491681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "vDxvKP1WKey4",
        "outputId": "370f2345-cfa4-43fe-d138-8c86e170ca14"
      },
      "source": [
        "#@title Input Spike Trains\n",
        "\n",
        "spk_freq = 10 # not sure about this, but assuming it since the paper\n",
        "# uses 10 Hz frequency as the target output frequency (actually \n",
        "# 5 equidistant spikes over 500 ms)\n",
        "\n",
        "input_trains = Poisson_trains(100, spk_freq*np.ones(100),\n",
        "                              nb_steps, timestep_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-6133751472fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m input_trains = Poisson_trains(100, spk_freq*np.ones(100),\n\u001b[0;32m----> 8\u001b[0;31m                               nb_steps, timestep_size)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-1eb48156c2f9>\u001b[0m in \u001b[0;36mPoisson_trains\u001b[0;34m(n, lam, timesteps, dt)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#  counter = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munif\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#    counter += len(unif <= lam[i]*dt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#  print(\"Total No. of Spikes\", counter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWtuFCfcKx-3"
      },
      "source": [
        "#@title Target Spike Trains\n",
        "\n",
        "target = torch.zeros(nb_steps)\n",
        "target[:: 1000] = 1\n",
        "#print(target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wyZRvOpEnoP",
        "outputId": "096f1944-714a-4e9f-eef2-40a91e78faf8"
      },
      "source": [
        "#@title Weight Initialization\n",
        "\n",
        "weight_scale = 7*(1 - beta) # copied from spytorch\n",
        "\n",
        "weights = torch.empty((nb_inputs, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(weights, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "print(\"Weight Initialization Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Initialization Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABN62sH9Ew9o"
      },
      "source": [
        "#@title SNN\n",
        "def run_snn(inputs):\n",
        "\n",
        "  h1 = torch.einsum('abc,cd->abd', (inputs,w1))\n",
        "  syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  # lists to record the membrane potentials and the synaptic currents:\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  # loop to simulate time\n",
        "  for t in range(nb_steps):\n",
        "    mthr = mem - 1.0\n",
        "    out = spike_fn(mthr)\n",
        "    rst = out.detach()  # do not want to backpropagate through reset\n",
        "\n",
        "    new_syn = alpha*syn + h1[:, t]\n",
        "    new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  # create tensors to stack the elements in the recording lists\n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  # readout layer\n",
        "  h2 = torch.einsum('abc,cd->abd', (spk_rec, w2))\n",
        "  flt = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out_rec = [out]\n",
        "  for t in range(nb_steps):\n",
        "    new_flt = alpha*flt + h2[:, t]\n",
        "    new_out = beta*out + flt\n",
        "\n",
        "    flt = new_flt\n",
        "    out = new_out\n",
        "\n",
        "    out_rec.append(out)\n",
        "\n",
        "  out_rec = torch.stack(out_rec, dim=1)\n",
        "  other_recs = [mem_rec, spk_rec]\n",
        "  return out_rec, other_recs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWFx62lD-Ihi"
      },
      "source": [
        "def run_single_neuron(input):\n",
        "\n",
        "  syn = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "  mem = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "\n",
        "  for t in range(nb_steps):\n",
        "\n",
        "    out = spike_fn(mem - 1) # since firing threshold is scaled to 1\n",
        "    reset = out.detach()\n",
        "\n",
        "    \n",
        "    new_syn = alpha*syn + torch.sum(input[:, t] * weights)\n",
        "    new_mem = (beta*mem + (1-beta)*new_syn)*(1 - reset)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  return spk_rec, mem_rec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCD2BlcDRgRf"
      },
      "source": [
        "def doubleExponential(train, args):\n",
        "  z = torch.zeros(len(train), device=device, dtype=dtype)\n",
        "  z_hat = torch.zeros(len(train), device=device, dtype=dtype)\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "  dt = timestep_size\n",
        "  for t in range(len(train) - 1):\n",
        "    z[t+1] =  z[t] + (-z[t]/t_rise + train[t])*dt\n",
        "    z_hat[t+1] = z_hat[t] + (-z_hat[t] + z[t])*dt/t_decay\n",
        "\n",
        "  return z_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB2wm-LoFnwC"
      },
      "source": [
        "def van_rossum_loss(spike_train, desired_spike_train, args):\n",
        "\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "\n",
        "  difference = doubleExponential(desired_spike_train,\n",
        "                                 args) - doubleExponential(spike_train,\n",
        "                                                           args)\n",
        "  loss = 1/2 * torch.sum(difference**2)*timestep_size \n",
        "\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcND5V8EmPps"
      },
      "source": [
        "very nice description of creating custom loss functions: https://pdf.co/blog/deep-learning-pytorch-custom-loss-function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49SLioKmGJt"
      },
      "source": [
        "class van_rossum_loss2(nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super().__init__()\n",
        "    self.t_rise = args['t_rise']\n",
        "    self.t_decay = args['t_decay']\n",
        "  \n",
        "  def forward(self, input, target):\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "vWkTnuZoFTx8",
        "outputId": "abbb0dd6-cfe9-412a-8d7b-5ddf16b66e5e"
      },
      "source": [
        "params = [weights]\n",
        "optimizer = torch.optim.Adam(params, lr=2e-3, betas=(0.9, 0.999))\n",
        "\n",
        "loss_hist = []\n",
        "\n",
        "for epochs in range(100):\n",
        "\n",
        "  output, _ = run_single_neuron(input_trains)\n",
        "  loss = van_rossum_loss(output, target, args)\n",
        " # loss = variable(van_rossum_loss, output, target, args)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  loss_hist.append(loss.item())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:249: UserWarning: torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\n",
            "  warnings.warn(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-35dccf622391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_single_neuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_trains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m  \u001b[0;31m# loss = van_rossum_loss(output, target, args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvan_rossum_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mARNXHlm14hQ"
      },
      "source": [
        "params = [weights] # The paramters we want to optimize\n",
        "optimizer = torch.optim.Adam(params, lr=2e-3, betas=(0.9,0.999)) # The optimizer we are going to use\n",
        "\n",
        "log_softmax_fn = nn.LogSoftmax(dim=1) # The log softmax function across output units\n",
        "loss_fn = nn.NLLLoss() # The negative log likelihood loss function\n",
        "\n",
        "# The optimization loop\n",
        "loss_hist = []\n",
        "for e in range(1000):\n",
        "    # run the network and get output\n",
        "    output,_ = run_snn(x_data) \n",
        "    # compute the loss\n",
        "    m,_= torch.max(output,1)\n",
        "    log_p_y = log_softmax_fn(m) \n",
        "    loss_val = loss_fn(log_p_y, y_data)\n",
        "\n",
        "    # update the weights\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # store loss value\n",
        "    loss_hist.append(loss_val.item())\n",
        "    \n",
        "loss_hist_true_grad = loss_hist # store for later use"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}