{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superspike implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddG8pHV0kwc"
      },
      "source": [
        "Link to paper: [Zenke, Ganguli - 2018](https://direct.mit.edu/neco/article/30/6/1514-1541/8378)\n",
        "\n",
        "Zenke's [Tutorial](https://github.com/fzenke/spytorch) on Surrogate Gradient Descent using PyTorch.\n",
        "\n",
        "To Implement:\n",
        "1. LIF Neurons (maybe a class of such neurons)\n",
        "2. Fast Sigmoid Function\n",
        "\n",
        "Question:\n",
        "1. How to implement spiking neural network in pytorch? \n",
        "  * use RNNs as Zenke suggests in his tutorial?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPL18elCNSBW"
      },
      "source": [
        "#@title Dependencies\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A42hX2QOHrHl"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Uncomment the line below to run on GPU\n",
        "# device = torch.device(\"cuda:0\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-EjYFuIet1"
      },
      "source": [
        "### Network Architecture\n",
        "\n",
        "3 layer feed-forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuJ0JQZbIoBS"
      },
      "source": [
        "nb_inputs  = 100\n",
        "nb_hidden  = 4\n",
        "nb_outputs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTj278ocJBr4"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtJfTLDHzj0"
      },
      "source": [
        "### Spiking Neuron Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qVMUOBIy0E"
      },
      "source": [
        "Since we are technically stimulating an RNN, the neurons have to be simulated for a certain number of timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMYsYrC0IyAA"
      },
      "source": [
        "time_step = 1e-3\n",
        "nb_steps  = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUhPO68HxiO"
      },
      "source": [
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTfvQ2wJP1R"
      },
      "source": [
        "Initializing weights from a normal distribution, the variance is scaled with the inverse square root of the number of input connections.\n",
        "\n",
        "Dale's Law is ignored here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoWnuuOJMMm"
      },
      "source": [
        "#@title Weight Matrcies\n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48tqmmkJwqv"
      },
      "source": [
        "#@title The Spiking Non-linearity\n",
        "def spike_fn(x):\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1.0\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbcmdidrSL4"
      },
      "source": [
        "h1 = torch.einsum(\"abc,cd->abd\", (x_data, w1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgY6EZFKhRZ"
      },
      "source": [
        "Initialize the synaptic currents and the membrane potentials at zero. Then implement a loop that stimulates the neuron models over time, and record the membrane potential and output spikes of all trials and all neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvUproZKUEA"
      },
      "source": [
        "# tensors initialized with zeros for synaptic current and membrane potential\n",
        "syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "# two lists to record the membrane potentials and output spikes\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# The simulation loop\n",
        "for t in range(nb_steps):\n",
        "  m_thr = mem - 1.0\n",
        "  out = spike_fn(m_thr)\n",
        "  rst = out.detach() # we do not want to backprop through the reset\n",
        "\n",
        "  new_syn = alpha*syn + h1[:, t]\n",
        "  new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "  mem_rec.append(mem)\n",
        "  spk_rec.append(out)\n",
        "\n",
        "  mem = new_mem\n",
        "  syn = new_syn\n",
        "\n",
        "mem_rec = torch.stack(mem_rec, dim=1)\n",
        "spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpfHuhVHs138"
      },
      "source": [
        "def run_snn(inputs):\n",
        "\n",
        "  h1 = torch.einsum('abc,cd->abd', (inputs,w1))\n",
        "  syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  # lists to record the membrane potentials and the synaptic currents:\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  # loop to simulate time\n",
        "  for t in range(nb_steps):\n",
        "    mthr = mem - 1.0\n",
        "    out = spike_fn(mthr)\n",
        "    rst = out.detach()  # do not want to backpropagate through reset\n",
        "\n",
        "    new_syn = alpha*syn + h1[:, t]\n",
        "    new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  # create tensors to stack the elements in the recording lists\n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  # readout layer\n",
        "  h2 = torch.einsum('abc,cd->abd', (spk_rec, w2))\n",
        "  flt = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out_rec = [out]\n",
        "  for t in range(nb_steps):\n",
        "    new_flt = alpha*flt + h2[:, t]\n",
        "    new_out = beta*out + flt\n",
        "\n",
        "    flt = new_flt\n",
        "    out = new_out\n",
        "\n",
        "    out_rec.append(out)\n",
        "\n",
        "  out_rec = torch.stack(out_rec, dim=1)\n",
        "  other_recs = [mem_rec, spk_rec]\n",
        "  return out_rec, other_recs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwc9l63f2nOn"
      },
      "source": [
        "### LIF Neuron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FuClaX2w0e"
      },
      "source": [
        "## SuperSpike Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jd4w5fENh7J"
      },
      "source": [
        "class SuperSpike(nn.Module):\n",
        "  def.__init__(self):\n",
        "    super(SuperSpike, self).__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlUxv4s413Q"
      },
      "source": [
        "#@title Surrogate Gradient\n",
        "\n",
        "class SurrGradSpike(torch.autograd.Function):\n",
        "\n",
        "  scale = 100.0 # controls the steepness of the gradient\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    '''\n",
        "    computes a step-function on the input. ctx is a context variable\n",
        "    that stores information needed later for backpropagation\n",
        "    '''\n",
        "    ctx.save_for_backward(input)\n",
        "    out = torch.zeros_like(input)\n",
        "    out[input > 0] = 1\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    '''\n",
        "    In the backward method, we recieve a tensor we need to compute \n",
        "    the surrogradient of the loss with respect to the input. \n",
        "    Here we use the negative half of the fast sigmoid as in \n",
        "    Zenke & Ganguli 2018.\n",
        "    \n",
        "    '''\n",
        "    input, _ = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "    return grad\n",
        "\n",
        "# overwrite the spike function with the surrograte gradient function\n",
        "# using the apply method\n",
        "spike_fn = SurrGradSpike.apply\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV65siTAc8sd"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "The van rossum distance is evaluated as:\n",
        "$$ L = (\\alpha*S_i - \\alpha*\\hat{S_i})^2 $$\n",
        "\n",
        "where $ \\alpha $ is a double exponential filter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWDywkDScAN"
      },
      "source": [
        "### Double Exponential Filter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cld4VdEcwY2"
      },
      "source": [
        "\n",
        "def doubleExponential(spike_train, dt, t_rise, t_decay, time):\n",
        "  \"\"\"\n",
        "  Implements the double exponential kernel\n",
        "  input:\n",
        "    spike_train - time series of spikes containing ones and zeros\n",
        "    dt\n",
        "    t_rise - time constant of first exponential filter\n",
        "    t_decay - time constant of second exponential filter\n",
        "    time - tuple with start & stop time (wrt spike_train) for the convolution\n",
        "  Returns:\n",
        "    the convolved double exponential product\n",
        "  \"\"\"\n",
        "  time_range = time[1] - time[0]\n",
        "\n",
        "  z = torch.zeros(time_range)\n",
        "  z_hat = torch.zeros(time_range)\n",
        "\n",
        "  for t in range(time_range):\n",
        "    z[t+1] =  z[t] + (-z[t] + spike_train[t])*dt\n",
        "    z_hat[t+1] = z_hat[t] + (-z_hat[t] + z[t])*dt\n",
        "\n",
        "  return z_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2GM8I4fUJv"
      },
      "source": [
        "### Hebbian Coincidence Detection & Synaptic Eligibility Trace\n",
        "\n",
        "$$ \\frac{dw_{ij}}{dt} = r\\int_{-\\infty}^t ds\\ e_i(s)\\ \\alpha * [\\sigma'(U_i(s))(\\epsilon*S_j(s))]  $$\n",
        "\n",
        "The evalutation of this equation requires:\n",
        "1. evaluation of presynaptic traces\n",
        "2. evaluation of hebbian coincidence and computation of the synaptic eligibility traces\n",
        "3. compuatation and propagation of error signals\n",
        "4. integration of this equation and weight update\n",
        "\n",
        "\n",
        "\n",
        "Here $ \\lambda_{ij} = \\sigma'(U_i(s))(\\epsilon*S_j(s)) $ is the eligibility trace.\n",
        "\n",
        "Fast sigmoid: $$   \\sigma(x) = \\frac{x}{1 + |x|} $$\n",
        "\n",
        "So, $ \\sigma'(U_i) = \\frac{1}{(1 + |h_i|)^2} $\n",
        "\n",
        "where $ h_i = \\beta(U_i - \\nu) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRfLDcodla6"
      },
      "source": [
        "def eligibility_trace(mem, spike_train, args):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    mem: membrane potential of i-th neuron for all \n",
        "         relevant timesteps, 1-D array shape: (timesteps)\n",
        "    spike_train: j-th neuron, 1-D array, shape: (timesteps)\n",
        "    thres: firing threshold\n",
        "  Returns:\n",
        "    eligibility trace\n",
        "  \"\"\"\n",
        "  beta = 1 # mV^-1\n",
        "  h_i = beta*(mem - thres)\n",
        "\n",
        "  post = 1 / (1 + torch.abs(h_i))**2 \n",
        "  pre_synaptic_trace = doubleExponential(spike_train, dt, t_rise, t_decay, time)\n",
        "\n",
        "  hebbian = post * pre_synaptic_trace #hebbian coincidence term\n",
        "\n",
        "  #synaptic eligibility trace\n",
        "  eligibility_trace = doubleExponential(hebbian, dt, t_rise, t_decay, time)\n",
        "\n",
        "  return elibility_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BATIjT7BlD1t"
      },
      "source": [
        "### Error Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fIjji7XOok"
      },
      "source": [
        "#@title Output Error Signal\n",
        "\n",
        "def error_signal(spike_train, desired_spike_train):\n",
        "  \"\"\"\n",
        "  Returns the error signal (time series)\n",
        "  \"\"\"\n",
        "  # output spike train\n",
        "  filter_1 = doubleExponential(spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  # desired spike train\n",
        "  filter_2 = doubleExponential(desired_spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  \n",
        "  error = filter_2 - filter_1\n",
        "  return error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0UfjVzQZti"
      },
      "source": [
        "#@title Feedback Signal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJ_QKUSTO0F"
      },
      "source": [
        "### Per Parameter Learning Rate\n",
        "\n",
        "Zenke & Ganguli (2018) used a per parameter learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDZpUAxTNOb"
      },
      "source": [
        "def learning_rate():\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJKFm5liSxym"
      },
      "source": [
        "### Poisson Spike Trains\n",
        "\n",
        "Based on the method suggested by David Heeger [here](https://www.cns.nyu.edu/~david/handouts/poisson.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6HSjSVcSDKA"
      },
      "source": [
        "def Poisson_trains(n, lam, timesteps, dt):\n",
        "  \"\"\"\n",
        "\n",
        "  inputs:\n",
        "    n - number of poisson spike trains \n",
        "    lam - 1-D array containing mean value of poisson trains\n",
        "  Returns\n",
        "\n",
        "  \"\"\"\n",
        "  trains = torch.zeros((n, timesteps))\n",
        "  unif = torch.rand((n, timesteps))\n",
        "  print(unif)\n",
        "  print(lam*dt)\n",
        "\n",
        "  counter = 0\n",
        "  for i in range(n):\n",
        "    trains[unif <= lam[i]*dt] = 1\n",
        "    counter += len(unif <= lam[i]*dt)\n",
        "  print(\"Total No. of Spikes\", counter)\n",
        "\n",
        "  return trains"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRVvazAW4264",
        "outputId": "4d7d55e9-be74-4889-c99c-8173720a29ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trains = Poisson_trains(1, 100*np.ones(10), 1000, 1e-4)\n",
        "\n",
        "print(trains)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.6941e-01, 9.2857e-01, 1.5926e-01, 3.2097e-01, 7.3991e-02, 8.5742e-01,\n",
            "         1.4587e-01, 3.3137e-01, 2.6911e-01, 4.5206e-02, 6.2814e-01, 8.9467e-01,\n",
            "         2.8943e-01, 2.0006e-01, 6.8276e-01, 4.9644e-02, 8.5288e-02, 7.7424e-01,\n",
            "         8.1146e-01, 8.1954e-01, 9.2050e-01, 7.3686e-01, 5.1565e-01, 6.5492e-01,\n",
            "         9.7569e-01, 1.6577e-01, 9.8835e-01, 8.2683e-02, 2.7187e-01, 5.7670e-01,\n",
            "         9.1205e-01, 9.0022e-01, 9.2279e-01, 4.6950e-01, 3.6623e-02, 1.4586e-01,\n",
            "         2.3351e-01, 2.4276e-01, 4.9419e-01, 3.6346e-01, 6.6140e-01, 7.9350e-01,\n",
            "         5.5390e-01, 2.0474e-01, 3.1089e-01, 5.3035e-01, 9.4335e-01, 4.4111e-01,\n",
            "         7.1835e-01, 9.4875e-02, 2.0452e-01, 4.7483e-01, 7.0753e-01, 4.3659e-01,\n",
            "         2.4363e-01, 7.6736e-02, 9.4806e-01, 9.7339e-01, 9.3424e-01, 3.8554e-01,\n",
            "         8.4655e-01, 2.1704e-01, 7.0620e-01, 4.7297e-01, 9.5011e-01, 7.8698e-01,\n",
            "         4.9152e-01, 3.8943e-01, 7.1294e-01, 6.8333e-01, 8.5256e-01, 6.0764e-01,\n",
            "         9.3572e-01, 4.0508e-02, 5.6576e-01, 3.2155e-01, 9.8335e-01, 8.8231e-01,\n",
            "         1.7082e-01, 4.2362e-01, 1.8367e-01, 1.9997e-01, 9.9492e-02, 5.8634e-01,\n",
            "         2.6916e-01, 3.2018e-01, 5.4135e-01, 8.6270e-01, 1.2997e-01, 6.3360e-01,\n",
            "         8.9034e-01, 3.2907e-01, 8.4758e-01, 4.0873e-01, 9.2286e-01, 5.6442e-01,\n",
            "         2.8679e-01, 3.8094e-01, 8.6290e-01, 7.6448e-01, 4.6216e-01, 5.9600e-01,\n",
            "         9.4307e-01, 8.0215e-02, 3.6771e-01, 1.6598e-01, 2.8394e-01, 1.5643e-02,\n",
            "         8.2380e-01, 8.0174e-01, 6.7786e-01, 9.1153e-01, 8.8941e-01, 7.1648e-01,\n",
            "         4.2358e-01, 7.5135e-01, 3.4738e-01, 9.3061e-01, 4.1424e-01, 5.7517e-01,\n",
            "         6.2094e-01, 8.3280e-01, 1.1488e-02, 8.0224e-01, 2.2370e-01, 8.1891e-01,\n",
            "         9.3247e-01, 6.2626e-01, 4.5976e-01, 2.2773e-01, 5.6540e-01, 3.0824e-01,\n",
            "         2.1072e-01, 8.4960e-01, 3.8814e-01, 2.0360e-01, 7.1321e-01, 2.6666e-01,\n",
            "         4.3575e-01, 2.3343e-01, 9.3563e-01, 7.8679e-01, 1.3159e-02, 4.1334e-01,\n",
            "         1.2359e-01, 5.8734e-01, 5.4943e-01, 2.4702e-01, 4.7528e-01, 7.6293e-01,\n",
            "         8.6640e-01, 4.0217e-01, 4.9501e-01, 1.1638e-01, 6.4396e-01, 2.1480e-01,\n",
            "         5.2087e-01, 7.7605e-01, 4.0608e-03, 4.4351e-01, 9.7848e-01, 9.6490e-02,\n",
            "         7.0565e-01, 4.8630e-02, 7.7812e-01, 8.9366e-01, 1.7246e-02, 8.7175e-01,\n",
            "         7.6485e-01, 6.2945e-01, 2.1386e-01, 8.1080e-01, 3.3460e-01, 5.2687e-01,\n",
            "         9.4941e-01, 5.1592e-01, 9.7781e-01, 2.8957e-01, 3.3046e-01, 1.1768e-01,\n",
            "         2.9279e-01, 4.1703e-01, 6.2781e-01, 1.0991e-01, 4.6448e-01, 8.9122e-01,\n",
            "         6.1884e-01, 5.6594e-01, 5.8823e-01, 3.2819e-01, 1.5001e-01, 9.4888e-01,\n",
            "         5.9578e-01, 8.0860e-01, 1.8314e-01, 4.3249e-01, 6.7833e-01, 7.5562e-01,\n",
            "         2.8499e-01, 8.2195e-01, 5.8135e-01, 4.4456e-01, 2.8791e-01, 3.1197e-01,\n",
            "         2.9956e-01, 2.8257e-02, 2.8659e-01, 3.1693e-01, 9.7922e-01, 8.5937e-01,\n",
            "         1.8219e-01, 9.3805e-02, 1.0653e-01, 5.8010e-01, 3.4473e-01, 7.5855e-01,\n",
            "         2.8303e-01, 7.6010e-02, 6.2827e-02, 2.4242e-01, 8.6486e-01, 3.5321e-02,\n",
            "         5.1669e-01, 5.5785e-01, 8.6095e-01, 4.2936e-01, 7.3596e-01, 9.9510e-01,\n",
            "         7.4925e-01, 1.6959e-01, 7.0330e-01, 2.8183e-01, 7.7869e-01, 9.4661e-01,\n",
            "         6.9830e-01, 8.4948e-01, 6.8132e-01, 8.1839e-01, 1.9710e-01, 8.0614e-01,\n",
            "         1.9308e-01, 1.8642e-01, 1.5561e-01, 5.6989e-01, 4.0475e-01, 8.0648e-01,\n",
            "         4.6076e-01, 3.5028e-01, 2.8575e-01, 9.4260e-01, 2.4737e-01, 8.0614e-01,\n",
            "         2.3772e-01, 2.5764e-02, 6.7326e-01, 4.9762e-01, 1.4125e-01, 8.0221e-01,\n",
            "         5.3326e-01, 2.5148e-02, 6.2678e-01, 6.8767e-01, 1.1329e-01, 1.5026e-01,\n",
            "         3.3871e-01, 8.5684e-01, 2.6699e-01, 3.0830e-01, 5.6526e-01, 6.0894e-01,\n",
            "         8.8507e-01, 9.2204e-01, 7.2728e-01, 8.6109e-01, 9.7650e-01, 1.1486e-01,\n",
            "         5.3689e-01, 2.6932e-01, 9.2409e-01, 8.9440e-01, 3.3634e-01, 4.5437e-01,\n",
            "         5.2337e-01, 4.6832e-01, 9.9765e-01, 1.7538e-01, 6.1038e-01, 5.0200e-01,\n",
            "         1.3011e-01, 7.2309e-01, 2.8723e-01, 3.3992e-01, 4.4583e-01, 9.8955e-01,\n",
            "         9.1850e-01, 6.5059e-01, 4.9268e-01, 9.4874e-01, 5.9438e-01, 7.6028e-01,\n",
            "         4.5759e-01, 4.8087e-01, 8.1231e-02, 1.8779e-01, 7.0836e-01, 6.7041e-01,\n",
            "         7.1856e-01, 9.1002e-01, 5.5051e-01, 4.5711e-01, 9.3588e-01, 4.3014e-01,\n",
            "         8.1110e-01, 5.8975e-01, 2.5603e-01, 9.3957e-01, 1.9830e-03, 5.0546e-01,\n",
            "         6.0062e-02, 8.8420e-01, 9.0141e-01, 3.5798e-01, 2.7666e-01, 7.7615e-01,\n",
            "         9.1292e-01, 9.8321e-01, 1.8522e-01, 2.1088e-01, 2.8316e-01, 8.3736e-01,\n",
            "         1.9845e-01, 6.2696e-01, 8.7181e-01, 3.3166e-01, 6.6106e-01, 4.8453e-01,\n",
            "         1.6260e-01, 7.5542e-01, 5.8848e-01, 9.4574e-01, 8.7303e-01, 3.5019e-01,\n",
            "         9.7400e-01, 7.1212e-01, 3.8466e-01, 1.3307e-01, 6.7681e-01, 1.9902e-01,\n",
            "         3.0183e-01, 8.8002e-01, 5.9606e-01, 3.2220e-01, 2.6963e-01, 3.2071e-02,\n",
            "         5.6900e-01, 4.4110e-01, 2.9458e-01, 3.4936e-01, 9.2017e-01, 5.3098e-01,\n",
            "         2.4809e-01, 8.7057e-01, 1.9471e-01, 8.0650e-01, 4.2111e-03, 9.0687e-01,\n",
            "         9.0409e-01, 4.7351e-01, 5.5484e-01, 1.0926e-02, 8.9548e-01, 8.5451e-01,\n",
            "         2.3062e-01, 4.0871e-01, 6.5279e-01, 3.0925e-01, 9.9059e-01, 7.7660e-01,\n",
            "         6.7743e-01, 4.5155e-01, 9.0705e-01, 5.1819e-01, 2.9632e-01, 2.6430e-01,\n",
            "         8.2991e-01, 8.8696e-02, 9.0811e-02, 1.2218e-01, 3.6086e-02, 7.4106e-01,\n",
            "         4.0378e-01, 9.9437e-01, 5.3911e-01, 6.3639e-01, 5.7621e-01, 3.6229e-01,\n",
            "         8.4928e-01, 7.6480e-01, 2.0083e-01, 4.9807e-01, 3.4620e-02, 4.6412e-01,\n",
            "         5.0913e-01, 1.2060e-01, 6.1754e-01, 6.8031e-01, 4.6612e-02, 1.4726e-01,\n",
            "         7.7648e-01, 8.1833e-01, 8.8810e-01, 1.4292e-01, 1.1679e-01, 2.4433e-01,\n",
            "         7.7160e-01, 6.8835e-01, 6.9195e-01, 8.6859e-01, 3.4934e-01, 5.5358e-01,\n",
            "         8.9601e-01, 2.4821e-02, 6.7084e-01, 4.0212e-01, 9.6486e-01, 4.2865e-01,\n",
            "         3.0575e-01, 6.8846e-01, 7.1352e-01, 6.3621e-01, 7.9028e-01, 2.6780e-01,\n",
            "         8.3531e-01, 6.7560e-01, 7.6588e-01, 7.0524e-01, 2.9251e-01, 1.9845e-02,\n",
            "         6.4961e-01, 3.4599e-01, 8.6327e-01, 3.7592e-01, 4.6784e-01, 4.9869e-01,\n",
            "         1.2015e-01, 7.6807e-01, 3.6871e-01, 7.8484e-01, 7.8114e-01, 5.0634e-01,\n",
            "         5.9068e-01, 3.1938e-01, 1.8866e-01, 3.4310e-01, 1.9983e-01, 3.7192e-01,\n",
            "         2.6722e-01, 1.3254e-02, 9.5963e-03, 9.2165e-01, 2.1526e-01, 3.4699e-01,\n",
            "         9.5410e-01, 1.6094e-01, 7.3223e-01, 8.9887e-01, 5.2128e-01, 5.6391e-01,\n",
            "         3.5993e-01, 4.5886e-01, 2.8449e-01, 4.3804e-01, 8.9785e-01, 1.9672e-01,\n",
            "         5.5941e-01, 7.3490e-01, 5.5203e-01, 5.0264e-01, 3.0370e-01, 1.9017e-01,\n",
            "         2.2477e-01, 4.7256e-01, 7.4068e-01, 6.1595e-01, 9.0512e-01, 1.9416e-01,\n",
            "         8.8564e-01, 5.3727e-01, 6.7345e-01, 2.3979e-01, 8.4656e-01, 2.3089e-01,\n",
            "         2.3238e-01, 4.3797e-01, 4.2998e-01, 4.8060e-01, 8.2112e-01, 6.3173e-01,\n",
            "         5.4170e-01, 5.4083e-01, 5.7339e-02, 6.1262e-01, 4.3130e-01, 4.8839e-01,\n",
            "         9.2672e-02, 6.1833e-01, 6.7000e-01, 2.8598e-01, 6.3305e-01, 8.1623e-01,\n",
            "         3.5279e-01, 9.1274e-01, 7.4406e-01, 9.9502e-01, 7.2375e-02, 9.0450e-01,\n",
            "         8.2389e-01, 1.5011e-01, 9.9853e-02, 2.8022e-01, 2.9872e-01, 1.9370e-01,\n",
            "         3.2458e-01, 6.7347e-01, 5.0664e-01, 7.0325e-02, 7.4176e-01, 2.3141e-01,\n",
            "         2.2013e-01, 5.0594e-01, 5.2332e-01, 4.4967e-02, 2.3801e-01, 3.9630e-01,\n",
            "         5.0129e-01, 2.2951e-01, 4.3675e-01, 9.6773e-01, 4.0376e-01, 1.0560e-01,\n",
            "         3.7953e-01, 3.7274e-01, 8.1043e-01, 3.6274e-01, 1.9066e-01, 5.2192e-01,\n",
            "         7.5827e-01, 4.6902e-02, 4.4447e-01, 2.9509e-01, 5.4005e-01, 6.4674e-01,\n",
            "         6.7923e-01, 9.6368e-01, 6.0391e-01, 7.9162e-02, 3.2019e-01, 5.4407e-01,\n",
            "         8.1149e-01, 3.7219e-01, 4.5541e-01, 9.0898e-01, 1.2879e-01, 7.9363e-01,\n",
            "         2.1246e-01, 9.5787e-01, 4.3484e-01, 2.4859e-01, 8.1952e-01, 2.3358e-03,\n",
            "         3.7884e-01, 4.4654e-01, 6.5776e-01, 7.6633e-01, 6.8275e-01, 8.1508e-01,\n",
            "         2.6001e-01, 2.9505e-01, 2.2858e-01, 5.8398e-02, 4.0804e-01, 3.8469e-02,\n",
            "         5.9304e-01, 9.1797e-01, 1.6764e-01, 1.6958e-01, 1.4704e-01, 1.8874e-01,\n",
            "         6.8031e-01, 8.3677e-01, 3.9651e-01, 2.8255e-01, 9.9710e-01, 3.2081e-01,\n",
            "         8.1008e-01, 8.8204e-01, 1.5889e-01, 5.8271e-01, 3.2435e-01, 4.4572e-02,\n",
            "         5.4367e-01, 5.5197e-01, 9.9774e-01, 5.5643e-01, 2.1188e-01, 1.6913e-01,\n",
            "         1.5000e-01, 2.0045e-01, 2.0659e-01, 9.3802e-01, 4.5381e-01, 4.4027e-01,\n",
            "         8.1717e-01, 3.0249e-01, 7.7205e-01, 2.0619e-01, 2.1747e-01, 6.3781e-01,\n",
            "         4.6276e-01, 5.1290e-01, 8.8916e-01, 7.3235e-01, 4.7934e-01, 7.7190e-01,\n",
            "         9.2363e-01, 1.0717e-02, 7.6695e-01, 8.2497e-01, 2.8328e-01, 7.1303e-01,\n",
            "         1.2062e-03, 9.3399e-02, 6.7307e-01, 9.3217e-01, 8.5906e-01, 5.8981e-01,\n",
            "         5.0329e-01, 7.5133e-01, 7.4593e-02, 7.0780e-01, 1.9412e-02, 8.9609e-01,\n",
            "         1.7217e-01, 8.3401e-01, 7.3121e-01, 3.4112e-01, 5.5716e-01, 1.3625e-01,\n",
            "         8.6424e-01, 8.6795e-01, 7.2791e-01, 9.3406e-01, 4.6621e-01, 2.1309e-01,\n",
            "         9.4314e-02, 4.6495e-01, 9.4686e-01, 2.1724e-01, 7.5196e-01, 7.5722e-01,\n",
            "         5.5875e-01, 7.3029e-01, 3.5383e-01, 5.2912e-01, 1.5181e-01, 3.1140e-01,\n",
            "         5.5522e-01, 9.2931e-01, 5.7765e-01, 5.7353e-01, 5.1746e-01, 6.3380e-01,\n",
            "         2.8195e-01, 7.5841e-01, 9.9099e-01, 1.8529e-01, 9.0730e-01, 4.5824e-01,\n",
            "         9.0952e-01, 5.1221e-01, 4.5682e-01, 4.9773e-01, 8.5145e-01, 6.8939e-01,\n",
            "         5.7005e-01, 2.9945e-02, 1.8566e-02, 8.8330e-01, 1.2022e-01, 2.6936e-02,\n",
            "         4.8495e-01, 4.2750e-01, 7.6102e-01, 5.4604e-01, 2.0697e-01, 2.1442e-01,\n",
            "         7.8115e-01, 1.7180e-01, 3.0047e-01, 4.8182e-01, 6.3514e-01, 4.7032e-01,\n",
            "         5.5444e-01, 7.2454e-01, 2.5359e-01, 3.7886e-01, 9.2830e-01, 2.0487e-01,\n",
            "         3.6924e-01, 4.2666e-01, 6.3710e-01, 9.5620e-02, 3.1322e-01, 2.5427e-01,\n",
            "         2.1994e-01, 8.5152e-02, 6.2651e-02, 2.2240e-01, 8.8266e-01, 7.2940e-01,\n",
            "         7.7071e-01, 9.3316e-01, 2.7952e-01, 4.3608e-01, 2.9945e-01, 6.8995e-02,\n",
            "         2.6406e-01, 8.4604e-01, 9.9245e-01, 6.0243e-01, 8.3163e-01, 4.0052e-02,\n",
            "         9.9995e-01, 6.0940e-01, 8.6665e-01, 1.7628e-01, 2.7549e-01, 8.0106e-01,\n",
            "         3.9078e-02, 8.0779e-01, 1.0652e-01, 9.1804e-01, 5.5980e-01, 7.1312e-01,\n",
            "         4.5103e-01, 7.0390e-01, 6.9132e-02, 3.0977e-01, 2.6628e-01, 6.9745e-01,\n",
            "         5.1965e-01, 3.9299e-01, 3.5340e-02, 4.6264e-01, 9.5156e-01, 7.7958e-02,\n",
            "         7.5942e-02, 8.4191e-01, 1.5812e-02, 7.6467e-01, 7.0776e-01, 6.6722e-01,\n",
            "         3.6913e-01, 4.6642e-01, 1.3615e-01, 5.5964e-01, 2.7541e-01, 6.3482e-01,\n",
            "         3.3141e-01, 9.4813e-04, 9.7066e-01, 3.0368e-01, 8.3764e-01, 3.3793e-01,\n",
            "         9.0674e-01, 8.7103e-01, 9.6861e-01, 8.6536e-01, 6.0693e-01, 5.1180e-03,\n",
            "         2.9919e-01, 5.0166e-01, 6.9068e-01, 5.5409e-01, 8.3926e-02, 1.9484e-01,\n",
            "         3.9077e-01, 3.0938e-01, 5.7917e-01, 6.3764e-02, 7.7550e-01, 5.0353e-01,\n",
            "         6.0900e-01, 7.2864e-01, 9.3734e-01, 4.5248e-01, 6.5035e-01, 4.5029e-01,\n",
            "         2.1809e-02, 2.2294e-01, 9.3869e-02, 3.9549e-01, 4.1536e-01, 9.2597e-01,\n",
            "         7.8451e-01, 7.9261e-01, 3.4347e-01, 7.1497e-02, 2.0648e-01, 6.3240e-01,\n",
            "         8.4620e-01, 9.9514e-01, 7.7253e-01, 5.6171e-01, 5.8484e-01, 6.4301e-01,\n",
            "         3.8273e-01, 7.9621e-01, 6.2475e-01, 7.1857e-01, 6.6892e-01, 1.2467e-02,\n",
            "         1.1113e-01, 2.5382e-01, 9.0882e-01, 4.0800e-01, 9.1923e-01, 4.3146e-02,\n",
            "         1.9097e-01, 9.6745e-01, 9.8524e-01, 8.5315e-01, 3.7690e-01, 7.1989e-01,\n",
            "         5.1561e-01, 4.8228e-01, 8.6673e-01, 4.9965e-01, 5.4487e-01, 1.1005e-01,\n",
            "         7.9155e-01, 4.8275e-01, 6.5788e-01, 5.5531e-01, 1.7260e-01, 9.2143e-01,\n",
            "         2.9357e-01, 1.4975e-01, 6.7414e-01, 6.8191e-01, 8.9218e-01, 5.3234e-01,\n",
            "         5.8238e-01, 9.4940e-01, 9.3938e-01, 2.5820e-01, 2.5526e-02, 8.3093e-01,\n",
            "         5.8685e-01, 6.3162e-01, 9.8186e-01, 2.7784e-02, 2.4233e-01, 3.2477e-01,\n",
            "         9.5713e-01, 1.5446e-01, 8.2850e-01, 6.8109e-01, 2.8724e-01, 4.1520e-01,\n",
            "         5.7037e-01, 5.9681e-01, 8.1179e-01, 9.6609e-02, 5.5591e-01, 6.1635e-01,\n",
            "         8.1273e-01, 7.9362e-01, 5.7750e-02, 7.6691e-01, 9.0029e-01, 1.7530e-01,\n",
            "         8.0249e-02, 8.6571e-01, 5.1714e-01, 6.5976e-01, 3.9552e-01, 2.3561e-01,\n",
            "         7.9274e-01, 2.5177e-01, 3.3691e-01, 2.4136e-01, 4.6577e-03, 2.5831e-01,\n",
            "         8.5684e-01, 7.6434e-01, 3.3201e-01, 6.5494e-01, 4.8322e-02, 3.8515e-01,\n",
            "         3.0142e-01, 1.4650e-01, 2.0163e-01, 5.3096e-01, 5.6717e-01, 6.9516e-01,\n",
            "         2.6076e-01, 6.3260e-01, 8.2399e-03, 6.4852e-02, 3.5810e-01, 5.9501e-02,\n",
            "         6.7770e-01, 9.3538e-01, 4.5009e-01, 4.3899e-01, 4.1915e-01, 3.2076e-01,\n",
            "         1.9981e-01, 4.0579e-02, 2.9305e-01, 5.9170e-01, 2.5081e-01, 2.0023e-02,\n",
            "         7.3290e-01, 6.4749e-01, 8.8722e-01, 4.6669e-01, 3.0561e-01, 9.5734e-01,\n",
            "         3.3106e-01, 3.2827e-01, 4.5378e-01, 7.5878e-01, 5.3626e-01, 9.6643e-01,\n",
            "         7.1133e-02, 3.7859e-01, 4.6389e-01, 1.0481e-01, 4.4109e-01, 8.1935e-01,\n",
            "         5.7886e-01, 3.4830e-01, 5.0172e-01, 1.4660e-02, 3.2354e-01, 6.8787e-01,\n",
            "         9.6821e-01, 7.7562e-01, 8.1460e-01, 3.4862e-01, 8.6138e-03, 3.2206e-01,\n",
            "         7.0211e-01, 9.8998e-01, 2.2250e-02, 1.5682e-01, 6.4531e-01, 6.8439e-01,\n",
            "         9.9652e-02, 9.3107e-01, 5.4642e-01, 9.2245e-01, 1.9351e-01, 3.2726e-01,\n",
            "         6.2796e-01, 8.9181e-02, 6.0282e-01, 9.8360e-01, 5.7872e-01, 8.9207e-01,\n",
            "         5.4072e-01, 9.3677e-01, 4.1832e-01, 4.2525e-01, 2.8904e-01, 5.5349e-01,\n",
            "         4.0814e-01, 3.0294e-01, 3.9853e-01, 1.6023e-02, 4.9068e-01, 9.9920e-01,\n",
            "         9.0808e-01, 4.5390e-01, 7.2924e-01, 3.3617e-01, 6.2601e-01, 2.1396e-01,\n",
            "         8.2657e-01, 4.5790e-01, 1.5813e-01, 9.1076e-01, 2.6538e-01, 1.6589e-02,\n",
            "         4.9312e-01, 4.9164e-01, 9.3589e-01, 5.5142e-01]])\n",
            "[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01]\n",
            "Total No. of Spikes 1\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoctDxVETK6F"
      },
      "source": [
        "#@title Step Function for Spikes\n",
        "def spike_fn(x, thres):\n",
        "  \"\"\"\n",
        "  Implements a heaviside function centred at the firing threshold\n",
        "  \"\"\"\n",
        "  x = x - thres\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDUPS2aHRRG7"
      },
      "source": [
        "### Single Neuron Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NatNAnH8RP0m"
      },
      "source": [
        "nb_inputs = 100 # 100 spike trains as inputs that repeat every 500 ms\n",
        "nb_outputs = 1 \n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "nb_steps = 5000\n",
        "timestep_size = 1e-4 # 0.1 msec timesteps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtO16h80uzyo"
      },
      "source": [
        "tau_syn = \n",
        "tay_mem = \n",
        "\n",
        "alpha = float(np.exp(timestep_size/tau_syn))\n",
        "beta = float(np.exp(timestep_size/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlHb3qGdeE9"
      },
      "source": [
        "#@title Input Spike Trains\n",
        "input_trains = Poisson_trains(100, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJP4zBk6T5F9"
      },
      "source": [
        "#@title Weight Initialization\n",
        "\n",
        "weight_scale = 7*(1 - beta) # copied from spytorch\n",
        "\n",
        "weights = torch.empty((nb_inputs, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.init.nn.normal_(weights, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U85Ydh-zdzBl"
      },
      "source": [
        "h1 = torch.einsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTjfwXjaZ0U"
      },
      "source": [
        "mem = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "spks = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "\n",
        "mem_rec = []\n",
        "spks_rec = []\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  for t in range(nb_steps):\n",
        "\n",
        "    h1 = torch.einsum('abd,dc->abc', ())\n",
        "\n",
        "    new_spk = alpha*spks + h1[:, t]\n",
        "    new_mem = beta*mem + \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}