{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superspike implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddG8pHV0kwc"
      },
      "source": [
        "Link to paper: [Zenke, Ganguli - 2018](https://direct.mit.edu/neco/article/30/6/1514-1541/8378)\n",
        "\n",
        "Zenke's [Tutorial](https://github.com/fzenke/spytorch) on Surrogate Gradient Descent using PyTorch.\n",
        "\n",
        "To Implement:\n",
        "1. LIF Neurons (maybe a class of such neurons)\n",
        "2. Fast Sigmoid Function\n",
        "\n",
        "Question:\n",
        "1. How to implement spiking neural network in pytorch? \n",
        "  * use RNNs as Zenke suggests in his tutorial?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPL18elCNSBW"
      },
      "source": [
        "#@title Dependencies\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from matplotlib.axes import Axes\n",
        "\n",
        "# import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A42hX2QOHrHl"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Uncomment the line below to run on GPU\n",
        "#device = torch.device(\"cuda:0\") "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-EjYFuIet1"
      },
      "source": [
        "### Network Architecture (Zenke)\n",
        "\n",
        "3 layer feed-forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuJ0JQZbIoBS"
      },
      "source": [
        "nb_inputs  = 100\n",
        "nb_hidden  = 4\n",
        "nb_outputs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTj278ocJBr4"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtJfTLDHzj0"
      },
      "source": [
        "### Spiking Neuron Model Setup (Zenke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qVMUOBIy0E"
      },
      "source": [
        "Since we are technically stimulating an RNN, the neurons have to be simulated for a certain number of timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMYsYrC0IyAA"
      },
      "source": [
        "time_step = 1e-3\n",
        "nb_steps  = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUhPO68HxiO"
      },
      "source": [
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTfvQ2wJP1R"
      },
      "source": [
        "Initializing weights from a normal distribution, the variance is scaled with the inverse square root of the number of input connections.\n",
        "\n",
        "Dale's Law is ignored here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoWnuuOJMMm"
      },
      "source": [
        "#@title Weight Matrcies\n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48tqmmkJwqv"
      },
      "source": [
        "#@title The Spiking Non-linearity\n",
        "def spike_fn(x):\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1.0\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbcmdidrSL4"
      },
      "source": [
        "h1 = torch.einsum(\"abc,cd->abd\", (x_data, w1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgY6EZFKhRZ"
      },
      "source": [
        "Initialize the synaptic currents and the membrane potentials at zero. Then implement a loop that stimulates the neuron models over time, and record the membrane potential and output spikes of all trials and all neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvUproZKUEA"
      },
      "source": [
        "# tensors initialized with zeros for synaptic current and membrane potential\n",
        "syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "# two lists to record the membrane potentials and output spikes\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# The simulation loop\n",
        "for t in range(nb_steps):\n",
        "  m_thr = mem - 1.0\n",
        "  out = spike_fn(m_thr)\n",
        "  rst = out.detach() # we do not want to backprop through the reset\n",
        "\n",
        "  new_syn = alpha*syn + h1[:, t]\n",
        "  new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "  mem_rec.append(mem)\n",
        "  spk_rec.append(out)\n",
        "\n",
        "  mem = new_mem\n",
        "  syn = new_syn\n",
        "\n",
        "mem_rec = torch.stack(mem_rec, dim=1)\n",
        "spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpfHuhVHs138"
      },
      "source": [
        "def run_snn(inputs):\n",
        "\n",
        "  h1 = torch.einsum('abc,cd->abd', (inputs,w1))\n",
        "  syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  # lists to record the membrane potentials and the synaptic currents:\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  # loop to simulate time\n",
        "  for t in range(nb_steps):\n",
        "    mthr = mem - 1.0\n",
        "    out = spike_fn(mthr)\n",
        "    rst = out.detach()  # do not want to backpropagate through reset\n",
        "\n",
        "    new_syn = alpha*syn + h1[:, t]\n",
        "    new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  # create tensors to stack the elements in the recording lists\n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  # readout layer\n",
        "  h2 = torch.einsum('abc,cd->abd', (spk_rec, w2))\n",
        "  flt = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out_rec = [out]\n",
        "  for t in range(nb_steps):\n",
        "    new_flt = alpha*flt + h2[:, t]\n",
        "    new_out = beta*out + flt\n",
        "\n",
        "    flt = new_flt\n",
        "    out = new_out\n",
        "\n",
        "    out_rec.append(out)\n",
        "\n",
        "  out_rec = torch.stack(out_rec, dim=1)\n",
        "  other_recs = [mem_rec, spk_rec]\n",
        "  return out_rec, other_recs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FuClaX2w0e"
      },
      "source": [
        "## SuperSpike Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jd4w5fENh7J"
      },
      "source": [
        "class SuperSpike(nn.Module):\n",
        "  def.__init__(self):\n",
        "    super(SuperSpike, self).__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlUxv4s413Q"
      },
      "source": [
        "#@title Surrogate Gradient\n",
        "\n",
        "class SurrGradSpike(torch.autograd.Function):\n",
        "\n",
        "  scale = 100.0 # controls the steepness of the gradient\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    '''\n",
        "    computes a step-function on the input. ctx is a context variable\n",
        "    that stores information needed later for backpropagation\n",
        "    '''\n",
        "    ctx.save_for_backward(input)\n",
        "    out = torch.zeros_like(input)\n",
        "    out[input > 0] = 1\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    '''\n",
        "    In the backward method, we recieve a tensor we need to compute \n",
        "    the surrogradient of the loss with respect to the input. \n",
        "    Here we use the negative half of the fast sigmoid as in \n",
        "    Zenke & Ganguli 2018.\n",
        "    \n",
        "    '''\n",
        "    input, _ = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "    return grad\n",
        "\n",
        "# overwrite the spike function with the surrograte gradient function\n",
        "# using the apply method\n",
        "spike_fn = SurrGradSpike.apply\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV65siTAc8sd"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "The van rossum distance is evaluated as:\n",
        "$$ L = (\\alpha*S_i - \\alpha*\\hat{S_i})^2 $$\n",
        "\n",
        "where $ \\alpha $ is a double exponential filter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWDywkDScAN"
      },
      "source": [
        "### Double Exponential Filter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cld4VdEcwY2"
      },
      "source": [
        "\n",
        "def doubleExponential(spike_train, dt, t_rise, t_decay, time):\n",
        "  \"\"\"\n",
        "  Implements the double exponential kernel\n",
        "  input:\n",
        "    spike_train - time series of spikes containing ones and zeros\n",
        "    dt\n",
        "    t_rise - time constant of first exponential filter\n",
        "    t_decay - time constant of second exponential filter\n",
        "    time - tuple with start & stop time (wrt spike_train) for the convolution\n",
        "  Returns:\n",
        "    the convolved double exponential product\n",
        "  \"\"\"\n",
        "  time_range = time[1] - time[0]\n",
        "\n",
        "  z = torch.zeros(time_range)\n",
        "  z_hat = torch.zeros(time_range)\n",
        "\n",
        "  for t in range(time_range):\n",
        "    z[t+1] =  z[t] + (-z[t]/t_rise + spike_train[t])*dt\n",
        "    z_hat[t+1] = z_hat[t] + (-z_hat[t] + z[t])*dt/t_decay\n",
        "\n",
        "  return z_hat"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2GM8I4fUJv"
      },
      "source": [
        "### Hebbian Coincidence Detection & Synaptic Eligibility Trace\n",
        "\n",
        "$$ \\frac{dw_{ij}}{dt} = r\\int_{-\\infty}^t ds\\ e_i(s)\\ \\alpha * [\\sigma'(U_i(s))(\\epsilon*S_j(s))]  $$\n",
        "\n",
        "The evalutation of this equation requires:\n",
        "1. evaluation of presynaptic traces\n",
        "2. evaluation of hebbian coincidence and computation of the synaptic eligibility traces\n",
        "3. compuatation and propagation of error signals\n",
        "4. integration of this equation and weight update\n",
        "\n",
        "\n",
        "\n",
        "Here $ \\lambda_{ij} = \\sigma'(U_i(s))(\\epsilon*S_j(s)) $ is the eligibility trace.\n",
        "\n",
        "Fast sigmoid: $$   \\sigma(x) = \\frac{x}{1 + |x|} $$\n",
        "\n",
        "So, $ \\sigma'(U_i) = \\frac{1}{(1 + |h_i|)^2} $\n",
        "\n",
        "where $ h_i = \\beta(U_i - \\nu) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_D7gVtIzIlX"
      },
      "source": [
        "def presynaptic_trace(value_exp1, value_exp2, spike, args):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    value_exp1 - 1-D array containing values of the single\n",
        "                 exponential trace at the previous timestep shape: (n, 1)\n",
        "    value_exp2 - 1-D array containing values of the second \n",
        "                 exponential trace at the previous timestep, shape: (n,1)\n",
        "    spike - 1-D array containing 0s or 1s for n presynaptic neurons\n",
        "    args['t_rise'] - \n",
        "    args['t_decay'] -\n",
        "  Returns:\n",
        "    the value of the presynaptic trace at the current timestep\n",
        "  \"\"\"\n",
        "  dt = args['timestep_size']\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "\n",
        "  #print(\"Presynaptic Traces value_exp1, shape\", value_exp1.shape)\n",
        "  #print(\"Presynaptic Traces value_exp2, shape\", value_exp2.shape)\n",
        "  \n",
        "  z = value_exp1 + (-value_exp1/t_rise + spike)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "  \n",
        "  return z, z_hat"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRfLDcodla6"
      },
      "source": [
        "def eligibility_trace(mem, spike_train, args):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    mem: membrane potential of i-th neuron for all \n",
        "         relevant timesteps, 1-D array shape: (timesteps)\n",
        "    spike_train: j-th neuron, 1-D array, shape: (timesteps)\n",
        "    thres: firing threshold\n",
        "  Returns:\n",
        "    eligibility trace\n",
        "  \"\"\"\n",
        "  thres = args['thres']\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "  \n",
        "  beta = 1 # mV^-1\n",
        "  h_i = beta*(mem - thres)\n",
        "\n",
        "  post = 1 / (1 + torch.abs(h_i))**2 \n",
        "  pre_synaptic_trace = doubleExponential(spike_train, dt, t_rise, t_decay, time)\n",
        "\n",
        "  hebbian = post * pre_synaptic_trace #hebbian coincidence term\n",
        "\n",
        "  #synaptic eligibility trace\n",
        "  eligibility_trace = doubleExponential(hebbian, dt, t_rise, t_decay, time)\n",
        "\n",
        "  return elibility_trace"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BATIjT7BlD1t"
      },
      "source": [
        "### Error Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fIjji7XOok"
      },
      "source": [
        "#@title Output Error Signal\n",
        "\n",
        "def error_signal(spike_train, desired_spike_train):\n",
        "  \"\"\"\n",
        "  Returns the error signal (time series)\n",
        "  \"\"\"\n",
        "  # output spike train\n",
        "  filter_1 = doubleExponential(spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  # desired spike train\n",
        "  filter_2 = doubleExponential(desired_spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  \n",
        "  error = filter_2 - filter_1\n",
        "  return error\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0epZHtN-DCY"
      },
      "source": [
        "def error_signal2(value_exp1, value_exp2, output, target, args):\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "  dt = args['timestep_size']\n",
        "\n",
        "  difference = target - output\n",
        "\n",
        "  z = value_exp1 + (-value_exp1/t_rise + difference)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "\n",
        "\n",
        "  return z, z_hat\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0UfjVzQZti"
      },
      "source": [
        "#@title Feedback Signal\n",
        "\n",
        "def feedback_signal():\n",
        "  \"\"\"\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  return feedback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJ_QKUSTO0F"
      },
      "source": [
        "### Per Parameter Learning Rate\n",
        "\n",
        "Zenke & Ganguli (2018) used a per parameter learning rate. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDZpUAxTNOb"
      },
      "source": [
        "def learning_rate():\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsyQm0lXdch"
      },
      "source": [
        "### Regularization Term\n",
        "\n",
        "Heterosynaptic regularization term to the learning rule of the hidden layers to avoid pathologically high firing rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXCD25nXg8k"
      },
      "source": [
        "def regularization_term():\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJKFm5liSxym"
      },
      "source": [
        "### Poisson Spike Trains\n",
        "\n",
        "Based on the method suggested by David Heeger [here](https://www.cns.nyu.edu/~david/handouts/poisson.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6HSjSVcSDKA"
      },
      "source": [
        "def Poisson_trains(n, lam, timesteps, dt):\n",
        "  \"\"\"\n",
        "\n",
        "  inputs:\n",
        "    n - number of poisson spike trains \n",
        "    lam - 1-D array containing mean value of poisson trains\n",
        "  Returns\n",
        "\n",
        "  \"\"\"\n",
        "  trains = torch.zeros((n, timesteps), device=device, dtype=dtype)\n",
        "  unif = torch.rand((n, timesteps), device=device, dtype=dtype)\n",
        "\n",
        "#  counter = 0\n",
        "  for i in range(n):\n",
        "    trains[unif <= lam[i]*dt] = 1\n",
        "#    counter += len(unif <= lam[i]*dt)\n",
        "#  print(\"Total No. of Spikes\", counter)\n",
        "\n",
        "  return trains"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRVvazAW4264"
      },
      "source": [
        "trains = Poisson_trains(1, 1000*np.ones(10), 1000, 1e-4)\n",
        "\n",
        "#print(trains)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoctDxVETK6F"
      },
      "source": [
        "#@title Step Function for Spikes\n",
        "def spike_fn(x, thres):\n",
        "  \"\"\"\n",
        "  Implements a heaviside function centred at the firing threshold\n",
        "  \"\"\"\n",
        "  x = x - thres\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1\n",
        "  return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoVT0g8hHomP"
      },
      "source": [
        "def van_rossum_loss(output, target, args):\n",
        "\n",
        "  z = 0\n",
        "  z_hat = 0\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "  dt = args['timestep_size']\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  #output = output.flatten(start_dim=1)\n",
        "\n",
        "  #print(\"Output Spike Train Shape:\", output.shape)\n",
        "  #print(\"Target Spike Train Shape:\", target.shape)\n",
        "\n",
        "  for i in range(len(output)):\n",
        "\n",
        "    difference = target[i] - output[i]\n",
        "  #  print(difference.shape)\n",
        "    z = z + (-z/t_rise + difference)*dt\n",
        "    z_hat = z_hat + (-z_hat + z)*dt/t_decay\n",
        "    \n",
        "    loss += dt*z_hat**2 \n",
        "\n",
        "  return (1/2)*loss\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7gcmMaHQif"
      },
      "source": [
        "def eligibility_trace2(value_exp1, value_exp2, hebbian, args):\n",
        "  \"\"\"\n",
        "  i: no. of presynaptic neurons\n",
        "  j: no. of postsynaptic neurons\n",
        "  Input:\n",
        "    value_exp1 - 2-D array containing values of the single exponential\n",
        "                 trace at the previous timestep. Shape: (i, j)\n",
        "    value_exp2 - 2-D array containing values of the second exponential\n",
        "                 trace at the previous timestep Shape: (i, j)\n",
        "    hebbian - 2-D array\n",
        "    args['t_rise_alpha']\n",
        "    args['t_decay_alpha']\n",
        "  Returns:\n",
        "\n",
        "  \"\"\"\n",
        "  dt = args['timestep_size']\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "\n",
        "#  print(\"last eligibility Value_exp1 shape:\", value_exp1.shape)\n",
        "#  print(\"last eligibility Value_exp2 shape:\", value_exp2.shape)\n",
        "#  print(\"hebbian shape\", hebbian.shape)\n",
        "\n",
        "  z = value_exp1 + (-value_exp1/t_rise + hebbian)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "\n",
        "  return z, z_hat"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDUPS2aHRRG7"
      },
      "source": [
        "### Single Neuron Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50_ZIOtqD9ov"
      },
      "source": [
        "Property | Value|\n",
        "-----| -----|\n",
        "threshold | -50 mV\n",
        "U_rest | -60 mV\n",
        "tau_mem| 10 ms\n",
        "tau_syn| 5 ms\n",
        "tau_ref| 5 ms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NatNAnH8RP0m"
      },
      "source": [
        "nb_inputs = 100 # 100 spike trains as inputs that repeat every 500 ms\n",
        "nb_outputs = 1 \n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "nb_steps = 5000\n",
        "timestep_size = 1e-4 # 0.1 msec timesteps\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XrBRxpDxME"
      },
      "source": [
        "# LIF Neuron Model Parameters\n",
        "args = {'thres': -50,\n",
        "        'U_rest': -60,\n",
        "        'tau_mem': 1e-2,\n",
        "        'tau_syn': 5e-3,\n",
        "        'tau_ref': 5e-3,\n",
        "        't_rise': 5e-3, # the pre-synaptic double exponential kernel rise time\n",
        "        't_decay': 1e-2, # the pre-synaptic double exponential kernel decay time\n",
        "        'timestep_size': 1e-4,\n",
        "        't_rise_alpha': 5e-3,\n",
        "        't_decay_alpha': 1e-2} "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtO16h80uzyo"
      },
      "source": [
        "tau_syn = args['tau_syn']\n",
        "tau_mem = args['tau_mem']\n",
        "\n",
        "alpha = float(np.exp(-timestep_size/tau_syn))\n",
        "beta = float(np.exp(-timestep_size/tau_mem))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "6e_2mHe4Ip6j",
        "outputId": "eea9abdd-a506-43e1-8238-112b13415695"
      },
      "source": [
        "def weight_update(input, output, target, mem, args):\n",
        "  update = torch.zeros(nb_outputs)\n",
        "\n",
        "  for i in range(nb_outputs):\n",
        "    update[i] = error_signal(output, target)*eligibility_trace(mem, input[i]\n",
        "  update = error_signal(output, target)*\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-76c5f9cbcee3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    update = error_signal(output, target)*\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlHb3qGdeE9"
      },
      "source": [
        "#@title Input Spike Trains\n",
        "\n",
        "spk_freq = 10 # not sure about this, but assuming it since the paper\n",
        "# uses 10 Hz frequency as the target output frequency (actually \n",
        "# 5 equidistant spikes over 500 ms)\n",
        "\n",
        "input_trains = Poisson_trains(100, spk_freq*np.ones(100),\n",
        "                              nb_steps, timestep_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmSXUKaCRXZT"
      },
      "source": [
        "#@title Target Spike Train\n",
        "## 5 equidistant spikes spread over 0.5 secs\n",
        "target = torch.zeros(nb_steps)\n",
        "target[:: nb_steps//5] = 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJP4zBk6T5F9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfef841b-549d-4199-c125-0ccd4f71ef09"
      },
      "source": [
        "#@title Weight Initialization\n",
        "\n",
        "weight_scale = 7*(1 - beta) # copied from spytorch\n",
        "\n",
        "weights = torch.empty((nb_inputs), device=device, dtype=dtype)\n",
        "torch.nn.init.normal_(weights, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "print(\"Weight initialization done\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight initialization done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTjfwXjaZ0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "6bbb7ffc-292d-405a-c864-e11c7a3d8209"
      },
      "source": [
        "mem = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "syn = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "thres = args['thres']\n",
        "\n",
        "loss_rec = []\n",
        "for i in tqdm(range(epochs)):\n",
        "\n",
        "  m = 0\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "\n",
        "  eligibility_trace_record = torch.zeros((nb_inputs, nb_steps), device=device, dtype=dtype)\n",
        " # pre_trace_rec = torch.zeros((nb_inputs, nb_inputs, nb_steps), device=device, dtype=dtype)\n",
        "  \n",
        "\n",
        "  eligibility_rec = []\n",
        "  pre_trace_rec = []\n",
        "\n",
        "  out_spks = torch.zeros(nb_steps, device=device, dtype=dtype)\n",
        "  \n",
        "  last_presynaptic_traces = [torch.zeros(nb_inputs), torch.zeros(nb_inputs)]\n",
        "  last_eligibility_traces = [torch.zeros(nb_inputs),\n",
        "                             torch.zeros(nb_inputs)] # not sure about the shape here, might have to recheck\n",
        "\n",
        "  last_error_values = [torch.zeros(nb_inputs),\n",
        "                            torch.zeros(nb_inputs)]\n",
        "  for t in range(nb_steps):\n",
        "\n",
        "    weighted_inp = input_trains[:, t] * weights\n",
        "\n",
        "    new_syn = alpha*syn + weighted_inp\n",
        "    new_mem = beta*mem + syn*(1 - beta)\n",
        "\n",
        "    mem_rec.append(new_mem)\n",
        "\n",
        "    out = spike_fn(new_mem, thres)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "\n",
        "    # compute presynaptic traces\n",
        "    presynaptic_traces = presynaptic_trace(last_presynaptic_traces[0],\n",
        "                                           last_presynaptic_traces[1],\n",
        "                                           input_trains[:, t], args)\n",
        "    \n",
        "    last_presynaptic_traces = presynaptic_traces\n",
        "    ##pre_trace_rec.append(presynaptic_traces)\n",
        "\n",
        "    # evaluate hebbian coincidence and synaptic eligibility traces\n",
        "    h = mem - thres\n",
        "    post = 1 / (1 + torch.abs(h))**2\n",
        "\n",
        "  \n",
        "\n",
        "    hebbian = post * presynaptic_traces[1]\n",
        "\n",
        "    synaptic_eligibility = eligibility_trace2(last_eligibility_traces[0],\n",
        "                                     last_eligibility_traces[1],\n",
        "                                     hebbian, args)\n",
        "\n",
        "    last_eligibility_traces = synaptic_eligibility\n",
        "\n",
        "  \n",
        "    error = error_signal2(last_error_values[0], last_error_values[1],\n",
        "                          out, target[t], args)\n",
        "    \n",
        "    last_error_values = error\n",
        "    # for minibatching weight updates in time\n",
        "  #  print(\"Error shape:\", error[1].shape)\n",
        "  #  print(\"Eligibility Shape:\", synaptic_eligibility[1].shape)\n",
        "\n",
        "    m += error[1] * synaptic_eligibility[1]\n",
        "  \n",
        "  #  print(\"\\n\")\n",
        "  print(m.shape)\n",
        "  weights += m\n",
        "  spk_rec = torch.stack(spk_rec, dim=0)\n",
        "  loss = van_rossum_loss(spk_rec, target, args)\n",
        "  \n",
        "  loss_rec.append(loss)\n",
        "\n",
        "  plt.\n",
        "\n",
        "print(loss_rec)\n",
        "\n",
        "\n",
        "plt.plot(loss_rec)\n",
        "    \n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:16,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:03<00:15,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:05<00:13,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:07<00:11,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:09<00:09,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:11<00:07,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:13<00:05,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:15<00:03,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:17<00:01,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n",
            "Output Spike Train Shape: torch.Size([5000, 1])\n",
            "Target Spike Train Shape: torch.Size([5000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:19<00:00,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06]), tensor([5.9813e-06])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc349809910>]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrKL8CLRqZfb"
      },
      "source": [
        "def raster_plot(spike_train, nb_steps, timestep_size):\n",
        "\n",
        "  positions = np.arange(0, nb_steps*timestep_size, timestep_size)\n",
        "  spike_positions = positions[spike_train == 1]\n",
        "  print(spike_positions)\n",
        "  Axes.eventplot(1, spike_positions)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCIK1uZytXic",
        "outputId": "0fd5058c-1799-46ab-ec53-8bc781c44885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "raster_plot(target, nb_steps, timestep_size)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.  0.1 0.2 0.3 0.4]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-e52b9e0fe0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraster_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-d984089271d6>\u001b[0m in \u001b[0;36mraster_plot\u001b[0;34m(spike_train, nb_steps, timestep_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mspike_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspike_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meventplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspike_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36meventplot\u001b[0;34m(self, positions, orientation, lineoffsets, linelengths, linewidths, colors, linestyles, **kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgallery\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlines_bars_and_markers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0meventplot_demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[0;32m-> 1282\u001b[0;31m         self._process_unit_info(xdata=positions,\n\u001b[0m\u001b[1;32m   1283\u001b[0m                                 \u001b[0mydata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineoffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinelengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                                 kwargs=kwargs)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute '_process_unit_info'"
          ]
        }
      ]
    }
  ]
}