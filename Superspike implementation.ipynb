{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superspike implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddG8pHV0kwc"
      },
      "source": [
        "Link to paper: [Zenke, Ganguli - 2018](https://direct.mit.edu/neco/article/30/6/1514-1541/8378)\n",
        "\n",
        "Zenke's [Tutorial](https://github.com/fzenke/spytorch) on Surrogate Gradient Descent using PyTorch.\n",
        "\n",
        "To Implement:\n",
        "1. LIF Neurons (maybe a class of such neurons)\n",
        "2. Fast Sigmoid Function\n",
        "\n",
        "Question:\n",
        "1. How to implement spiking neural network in pytorch? \n",
        "  * use RNNs as Zenke suggests in his tutorial?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPL18elCNSBW"
      },
      "source": [
        "#@title Dependencies\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A42hX2QOHrHl"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Uncomment the line below to run on GPU\n",
        "# device = torch.device(\"cuda:0\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-EjYFuIet1"
      },
      "source": [
        "### Network Architecture\n",
        "\n",
        "3 layer feed-forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuJ0JQZbIoBS"
      },
      "source": [
        "nb_inputs  = 100\n",
        "nb_hidden  = 4\n",
        "nb_outputs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTj278ocJBr4"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtJfTLDHzj0"
      },
      "source": [
        "### Spiking Neuron Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qVMUOBIy0E"
      },
      "source": [
        "Since we are technically stimulating an RNN, the neurons have to be simulated for a certain number of timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMYsYrC0IyAA"
      },
      "source": [
        "time_step = 1e-3\n",
        "nb_steps  = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUhPO68HxiO"
      },
      "source": [
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTfvQ2wJP1R"
      },
      "source": [
        "Initializing weights from a normal distribution, the variance is scaled with the inverse square root of the number of input connections.\n",
        "\n",
        "Dale's Law is ignored here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoWnuuOJMMm"
      },
      "source": [
        "#@title Weight Matrcies\n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48tqmmkJwqv"
      },
      "source": [
        "#@title The Spiking Non-linearity\n",
        "def spike_fn(x):\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1.0\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbcmdidrSL4"
      },
      "source": [
        "h1 = torch.einsum(\"abc,cd->abd\", (x_data, w1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgY6EZFKhRZ"
      },
      "source": [
        "Initialize the synaptic currents and the membrane potentials at zero. Then implement a loop that stimulates the neuron models over time, and record the membrane potential and output spikes of all trials and all neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvUproZKUEA"
      },
      "source": [
        "# tensors initialized with zeros for synaptic current and membrane potential\n",
        "syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "# two lists to record the membrane potentials and output spikes\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# The simulation loop\n",
        "for t in range(nb_steps):\n",
        "  m_thr = mem - 1.0\n",
        "  out = spike_fn(m_thr)\n",
        "  rst = out.detach() # we do not want to backprop through the reset\n",
        "\n",
        "  new_syn = alpha*syn + h1[:, t]\n",
        "  new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "  mem_rec.append(mem)\n",
        "  spk_rec.append(out)\n",
        "\n",
        "  mem = new_mem\n",
        "  syn = new_syn\n",
        "\n",
        "mem_rec = torch.stack(mem_rec, dim=1)\n",
        "spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpfHuhVHs138"
      },
      "source": [
        "def run_snn(inputs):\n",
        "\n",
        "  h1 = torch.einsum('abc,cd->abd', (inputs,w1))\n",
        "  syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  # lists to record the membrane potentials and the synaptic currents:\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  # loop to simulate time\n",
        "  for t in range(nb_steps):\n",
        "    mthr = mem - 1.0\n",
        "    out = spike_fn(mthr)\n",
        "    rst = out.detach()  # do not want to backpropagate through reset\n",
        "\n",
        "    new_syn = alpha*syn + h1[:, t]\n",
        "    new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  # create tensors to stack the elements in the recording lists\n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  # readout layer\n",
        "  h2 = torch.einsum('abc,cd->abd', (spk_rec, w2))\n",
        "  flt = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out_rec = [out]\n",
        "  for t in range(nb_steps):\n",
        "    new_flt = alpha*flt + h2[:, t]\n",
        "    new_out = beta*out + flt\n",
        "\n",
        "    flt = new_flt\n",
        "    out = new_out\n",
        "\n",
        "    out_rec.append(out)\n",
        "\n",
        "  out_rec = torch.stack(out_rec, dim=1)\n",
        "  other_recs = [mem_rec, spk_rec]\n",
        "  return out_rec, other_recs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwc9l63f2nOn"
      },
      "source": [
        "### LIF Neuron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FuClaX2w0e"
      },
      "source": [
        "## SuperSpike Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jd4w5fENh7J"
      },
      "source": [
        "class SuperSpike(nn.Module):\n",
        "  def.__init__(self):\n",
        "    super(SuperSpike, self).__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlUxv4s413Q"
      },
      "source": [
        "#@title Surrogate Gradient\n",
        "\n",
        "class SurrGradSpike(torch.autograd.Function):\n",
        "\n",
        "  scale = 100.0 # controls the steepness of the gradient\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    '''\n",
        "    computes a step-function on the input. ctx is a context variable\n",
        "    that stores information needed later for backpropagation\n",
        "    '''\n",
        "    ctx.save_for_backward(input)\n",
        "    out = torch.zeros_like(input)\n",
        "    out[input > 0] = 1\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    '''\n",
        "    In the backward method, we recieve a tensor we need to compute \n",
        "    the surrogradient of the loss with respect to the input. \n",
        "    Here we use the negative half of the fast sigmoid as in \n",
        "    Zenke & Ganguli 2018.\n",
        "    \n",
        "    '''\n",
        "    input, _ = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "    return grad\n",
        "\n",
        "# overwrite the spike function with the surrograte gradient function\n",
        "# using the apply method\n",
        "spike_fn = SurrGradSpike.apply\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV65siTAc8sd"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "The van rossum distance is evaluated as:\n",
        "$$ L = (\\alpha*S_i - \\alpha*\\hat{S_i})^2 $$\n",
        "\n",
        "where $ \\alpha $ is a double exponential filter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWDywkDScAN"
      },
      "source": [
        "### Double Exponential Filter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cld4VdEcwY2"
      },
      "source": [
        "\n",
        "def doubleExponential(spike_train, dt, t_rise, t_decay, time):\n",
        "  \"\"\"\n",
        "  Implements the double exponential kernel\n",
        "  input:\n",
        "    spike_train - time series of spikes containing ones and zeros\n",
        "    dt\n",
        "    t_rise - time constant of first exponential filter\n",
        "    t_decay - time constant of second exponential filter\n",
        "    time - tuple with start & stop time (wrt spike_train) for the convolution\n",
        "  Returns:\n",
        "    the convolved double exponential product\n",
        "  \"\"\"\n",
        "  time_range = time[1] - time[0]\n",
        "\n",
        "  z = torch.zeros(time_range)\n",
        "  z_hat = torch.zeros(time_range)\n",
        "\n",
        "  for t in range(time_range):\n",
        "    z[t+1] =  z[t] + (-z[t] + spike_train[t])*dt\n",
        "    z_hat[t+1] = z_hat[t] + (-z_hat[t] + z[t])*dt\n",
        "\n",
        "  return z_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2GM8I4fUJv"
      },
      "source": [
        "### Hebbian Coincidence Detection & Synaptic Eligibility Trace\n",
        "\n",
        "$$ \\frac{dw_{ij}}{dt} = r\\int_{-\\infty}^t ds\\ e_i(s)\\ \\alpha * [\\sigma'(U_i(s))(\\epsilon*S_j(s))]  $$\n",
        "\n",
        "The evalutation of this equation requires:\n",
        "1. evaluation of presynaptic traces\n",
        "2. evaluation of hebbian coincidence and computation of the synaptic eligibility traces\n",
        "3. compuatation and propagation of error signals\n",
        "4. integration of this equation and weight update\n",
        "\n",
        "\n",
        "\n",
        "Here $ \\lambda_{ij} = \\sigma'(U_i(s))(\\epsilon*S_j(s)) $ is the eligibility trace.\n",
        "\n",
        "Fast sigmoid: $$   \\sigma(x) = \\frac{x}{1 + |x|} $$\n",
        "\n",
        "So, $ \\sigma'(U_i) = \\frac{1}{(1 + |h_i|)^2} $\n",
        "\n",
        "where $ h_i = \\beta(U_i - \\nu) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRfLDcodla6"
      },
      "source": [
        "def eligibility_trace(mem, spike_train, args):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    mem: membrane potential of i-th neuron for all \n",
        "         relevant timesteps, 1-D array shape: (timesteps)\n",
        "    spike_train: j-th neuron, 1-D array, shape: (timesteps)\n",
        "    thres: firing threshold\n",
        "  Returns:\n",
        "    eligibility trace\n",
        "  \"\"\"\n",
        "  beta = 1 # mV^-1\n",
        "  h_i = beta*(mem - thres)\n",
        "\n",
        "  post = 1 / (1 + torch.abs(h_i))**2 \n",
        "  pre_synaptic_trace = doubleExponential(spike_train, dt, t_rise, t_decay, time)\n",
        "\n",
        "  hebbian = post * pre_synaptic_trace #hebbian coincidence term\n",
        "\n",
        "  #synaptic eligibility trace\n",
        "  eligibility_trace = doubleExponential(hebbian, dt, t_rise, t_decay, time)\n",
        "\n",
        "  return elibility_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BATIjT7BlD1t"
      },
      "source": [
        "### Error Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fIjji7XOok"
      },
      "source": [
        "#@title Output Error Signal\n",
        "\n",
        "def error_signal(spike_train, desired_spike_train):\n",
        "  \"\"\"\n",
        "  Returns the error signal (time series)\n",
        "  \"\"\"\n",
        "  # output spike train\n",
        "  filter_1 = doubleExponential(spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  # desired spike train\n",
        "  filter_2 = doubleExponential(desired_spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  \n",
        "  error = filter_2 - filter_1\n",
        "  return error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0UfjVzQZti"
      },
      "source": [
        "#@title Feedback Signal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJ_QKUSTO0F"
      },
      "source": [
        "### Per Parameter Learning Rate\n",
        "\n",
        "Zenke & Ganguli (2018) used a per parameter learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDZpUAxTNOb"
      },
      "source": [
        "def learning_rate():\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJKFm5liSxym"
      },
      "source": [
        "### Poisson Spike Trains\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6HSjSVcSDKA"
      },
      "source": [
        "\n",
        "def Poisson_trains(n, lam, timesteps):\n",
        "  \"\"\"\n",
        "  inputs:\n",
        "    n - number of poisson spike trains \n",
        "    lam - 1-D array containing mean value of poisson trains\n",
        "  Returns\n",
        "\n",
        "  \"\"\"\n",
        "  trains = torch.zeros((n, timesteps))\n",
        "  \n",
        "  torch.poisson()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoctDxVETK6F"
      },
      "source": [
        "#@title Step Function for Spikes\n",
        "def spike_fn(x, thres):\n",
        "  \"\"\"\n",
        "  Implements a heaviside function centred at the firing threshold\n",
        "  \"\"\"\n",
        "  x = x - thres\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDUPS2aHRRG7"
      },
      "source": [
        "### Single Neuron Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NatNAnH8RP0m"
      },
      "source": [
        "nb_inputs = 100 # 100 spike trains as inputs that repeat every 500 ms\n",
        "nb_outputs = 1 \n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "nb_steps = 5000\n",
        "timestep_size = 1e-4 # 0.1 msec timesteps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlHb3qGdeE9"
      },
      "source": [
        "#@title Input Spike Trains\n",
        "input_trains = Poisson_trains(100, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJP4zBk6T5F9"
      },
      "source": [
        "#@title Weight Initialization\n",
        "\n",
        "beta = np.exp() \n",
        "weight_scale = 7*(1 - beta) # copied from spytorch\n",
        "\n",
        "weights = torch.empty((nb_inputs, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.init.nn.normal_(weights, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U85Ydh-zdzBl"
      },
      "source": [
        "h1 = torch.einsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTjfwXjaZ0U"
      },
      "source": [
        "mem = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "spks = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "\n",
        "mem_rec = []\n",
        "spks_rec = []\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  for t in range(nb_steps):\n",
        "\n",
        "    new_mem = beta*mem + h1[:, t]\n",
        "    new_spk = \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}