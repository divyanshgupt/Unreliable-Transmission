{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superspike implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddG8pHV0kwc"
      },
      "source": [
        "Link to paper: [Zenke, Ganguli - 2018](https://direct.mit.edu/neco/article/30/6/1514-1541/8378)\n",
        "\n",
        "Zenke's [Tutorial](https://github.com/fzenke/spytorch) on Surrogate Gradient Descent using PyTorch.\n",
        "\n",
        "To Implement:\n",
        "1. LIF Neurons (maybe a class of such neurons)\n",
        "2. Fast Sigmoid Function\n",
        "\n",
        "Question:\n",
        "1. How to implement spiking neural network in pytorch? \n",
        "  * use RNNs as Zenke suggests in his tutorial?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPL18elCNSBW"
      },
      "source": [
        "#@title Dependencies\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A42hX2QOHrHl"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Uncomment the line below to run on GPU\n",
        "#device = torch.device(\"cuda:0\") "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-EjYFuIet1"
      },
      "source": [
        "### Network Architecture (Zenke)\n",
        "\n",
        "3 layer feed-forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuJ0JQZbIoBS"
      },
      "source": [
        "nb_inputs  = 100\n",
        "nb_hidden  = 4\n",
        "nb_outputs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTj278ocJBr4"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtJfTLDHzj0"
      },
      "source": [
        "### Spiking Neuron Model Setup (Zenke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qVMUOBIy0E"
      },
      "source": [
        "Since we are technically stimulating an RNN, the neurons have to be simulated for a certain number of timesteps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMYsYrC0IyAA"
      },
      "source": [
        "time_step = 1e-3\n",
        "nb_steps  = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUhPO68HxiO"
      },
      "source": [
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeTfvQ2wJP1R"
      },
      "source": [
        "Initializing weights from a normal distribution, the variance is scaled with the inverse square root of the number of input connections.\n",
        "\n",
        "Dale's Law is ignored here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoWnuuOJMMm"
      },
      "source": [
        "#@title Weight Matrcies\n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48tqmmkJwqv"
      },
      "source": [
        "#@title The Spiking Non-linearity\n",
        "def spike_fn(x):\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1.0\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbcmdidrSL4"
      },
      "source": [
        "h1 = torch.einsum(\"abc,cd->abd\", (x_data, w1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgY6EZFKhRZ"
      },
      "source": [
        "Initialize the synaptic currents and the membrane potentials at zero. Then implement a loop that stimulates the neuron models over time, and record the membrane potential and output spikes of all trials and all neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvUproZKUEA"
      },
      "source": [
        "# tensors initialized with zeros for synaptic current and membrane potential\n",
        "syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "# two lists to record the membrane potentials and output spikes\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# The simulation loop\n",
        "for t in range(nb_steps):\n",
        "  m_thr = mem - 1.0\n",
        "  out = spike_fn(m_thr)\n",
        "  rst = out.detach() # we do not want to backprop through the reset\n",
        "\n",
        "  new_syn = alpha*syn + h1[:, t]\n",
        "  new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "  mem_rec.append(mem)\n",
        "  spk_rec.append(out)\n",
        "\n",
        "  mem = new_mem\n",
        "  syn = new_syn\n",
        "\n",
        "mem_rec = torch.stack(mem_rec, dim=1)\n",
        "spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpfHuhVHs138"
      },
      "source": [
        "def run_snn(inputs):\n",
        "\n",
        "  h1 = torch.einsum('abc,cd->abd', (inputs,w1))\n",
        "  syn = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  mem = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "  # lists to record the membrane potentials and the synaptic currents:\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  # loop to simulate time\n",
        "  for t in range(nb_steps):\n",
        "    mthr = mem - 1.0\n",
        "    out = spike_fn(mthr)\n",
        "    rst = out.detach()  # do not want to backpropagate through reset\n",
        "\n",
        "    new_syn = alpha*syn + h1[:, t]\n",
        "    new_mem = (beta*mem + syn)(1 - rst)\n",
        "\n",
        "    mem_rec.append(mem)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "    mem = new_mem\n",
        "    syn = new_syn\n",
        "  \n",
        "  # create tensors to stack the elements in the recording lists\n",
        "  mem_rec = torch.stack(mem_rec, dim=1)\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "\n",
        "  # readout layer\n",
        "  h2 = torch.einsum('abc,cd->abd', (spk_rec, w2))\n",
        "  flt = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out = torch.zeros((batch_size, nb_outputs), device=device, dtype=dtype)\n",
        "  out_rec = [out]\n",
        "  for t in range(nb_steps):\n",
        "    new_flt = alpha*flt + h2[:, t]\n",
        "    new_out = beta*out + flt\n",
        "\n",
        "    flt = new_flt\n",
        "    out = new_out\n",
        "\n",
        "    out_rec.append(out)\n",
        "\n",
        "  out_rec = torch.stack(out_rec, dim=1)\n",
        "  other_recs = [mem_rec, spk_rec]\n",
        "  return out_rec, other_recs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FuClaX2w0e"
      },
      "source": [
        "## SuperSpike Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jd4w5fENh7J"
      },
      "source": [
        "class SuperSpike(nn.Module):\n",
        "  def.__init__(self):\n",
        "    super(SuperSpike, self).__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "\n",
        "  def forward(self, x):\n",
        "    i\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlUxv4s413Q"
      },
      "source": [
        "#@title Surrogate Gradient\n",
        "\n",
        "class SurrGradSpike(torch.autograd.Function):\n",
        "\n",
        "  scale = 100.0 # controls the steepness of the gradient\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    '''\n",
        "    computes a step-function on the input. ctx is a context variable\n",
        "    that stores information needed later for backpropagation\n",
        "    '''\n",
        "    ctx.save_for_backward(input)\n",
        "    out = torch.zeros_like(input)\n",
        "    out[input > 0] = 1\n",
        "    return out\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    '''\n",
        "    In the backward method, we recieve a tensor we need to compute \n",
        "    the surrogradient of the loss with respect to the input. \n",
        "    Here we use the negative half of the fast sigmoid as in \n",
        "    Zenke & Ganguli 2018.\n",
        "    \n",
        "    '''\n",
        "    input, _ = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "    return grad\n",
        "\n",
        "# overwrite the spike function with the surrograte gradient function\n",
        "# using the apply method\n",
        "spike_fn = SurrGradSpike.apply\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV65siTAc8sd"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "The van rossum distance is evaluated as:\n",
        "$$ L = (\\alpha*S_i - \\alpha*\\hat{S_i})^2 $$\n",
        "\n",
        "where $ \\alpha $ is a double exponential filter. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWDywkDScAN"
      },
      "source": [
        "### Double Exponential Filter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cld4VdEcwY2"
      },
      "source": [
        "\n",
        "def doubleExponential(spike_train, dt, t_rise, t_decay, time):\n",
        "  \"\"\"\n",
        "  Implements the double exponential kernel\n",
        "  input:\n",
        "    spike_train - time series of spikes containing ones and zeros\n",
        "    dt\n",
        "    t_rise - time constant of first exponential filter\n",
        "    t_decay - time constant of second exponential filter\n",
        "    time - tuple with start & stop time (wrt spike_train) for the convolution\n",
        "  Returns:\n",
        "    the convolved double exponential product\n",
        "  \"\"\"\n",
        "  time_range = time[1] - time[0]\n",
        "\n",
        "  z = torch.zeros(time_range)\n",
        "  z_hat = torch.zeros(time_range)\n",
        "\n",
        "  for t in range(time_range):\n",
        "    z[t+1] =  z[t] + (-z[t]/t_rise + spike_train[t])*dt\n",
        "    z_hat[t+1] = z_hat[t] + (-z_hat[t] + z[t])*dt/t_decay\n",
        "\n",
        "  return z_hat"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP2GM8I4fUJv"
      },
      "source": [
        "### Hebbian Coincidence Detection & Synaptic Eligibility Trace\n",
        "\n",
        "$$ \\frac{dw_{ij}}{dt} = r\\int_{-\\infty}^t ds\\ e_i(s)\\ \\alpha * [\\sigma'(U_i(s))(\\epsilon*S_j(s))]  $$\n",
        "\n",
        "The evalutation of this equation requires:\n",
        "1. evaluation of presynaptic traces\n",
        "2. evaluation of hebbian coincidence and computation of the synaptic eligibility traces\n",
        "3. compuatation and propagation of error signals\n",
        "4. integration of this equation and weight update\n",
        "\n",
        "\n",
        "\n",
        "Here $ \\lambda_{ij} = \\sigma'(U_i(s))(\\epsilon*S_j(s)) $ is the eligibility trace.\n",
        "\n",
        "Fast sigmoid: $$   \\sigma(x) = \\frac{x}{1 + |x|} $$\n",
        "\n",
        "So, $ \\sigma'(U_i) = \\frac{1}{(1 + |h_i|)^2} $\n",
        "\n",
        "where $ h_i = \\beta(U_i - \\nu) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_D7gVtIzIlX"
      },
      "source": [
        "def presynaptic_trace(value_exp1, value_exp2, spike, args):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    value_exp1 - 1-D array containing values of the single\n",
        "                 exponential trace at the previous timestep shape: (n, 1)\n",
        "    value_exp2 - 1-D array containing values of the second \n",
        "                 exponential trace at the previous timestep, shape: (n,1)\n",
        "    spike - 1-D array containing 0s or 1s for n presynaptic neurons\n",
        "    args['t_rise'] - \n",
        "    args['t_decay'] -\n",
        "  Returns:\n",
        "    the value of the presynaptic trace at the current timestep\n",
        "  \"\"\"\n",
        "  dt = args['timestep_size']\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "\n",
        "  z = value_exp1 + (-value_exp1/t_rise + spike)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "  \n",
        "  return z, z_hat"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7gcmMaHQif"
      },
      "source": [
        "def eligibility_trace2(value_exp1, value_exp2, hebbian, args):\n",
        "  \"\"\"\n",
        "  i: no. of presynaptic neurons\n",
        "  j: no. of postsynaptic neurons\n",
        "  Input:\n",
        "    value_exp1 - 2-D array containing values of the single exponential\n",
        "                 trace at the previous timestep. Shape: (i, j)\n",
        "    value_exp2 - 2-D array containing values of the second exponential\n",
        "                 trace at the previous timestep Shape: (i, j)\n",
        "    hebbian - 2-D array\n",
        "    args['t_rise_alpha']\n",
        "    args['t_decay_alpha']\n",
        "  Returns:\n",
        "\n",
        "  \"\"\"\n",
        "  dt = args['timestep_size']\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "\n",
        "  z = value_exp1 + (-value_exp1/t_rise + hebbian)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "\n",
        "  return z, z_hat"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRfLDcodla6"
      },
      "source": [
        "def eligibility_trace(mem, spike_train, args):\n",
        "  \"\"\"\n",
        "  input:\n",
        "    mem: membrane potential of i-th neuron for all \n",
        "         relevant timesteps, 1-D array shape: (timesteps)\n",
        "    spike_train: j-th neuron, 1-D array, shape: (timesteps)\n",
        "    thres: firing threshold\n",
        "  Returns:\n",
        "    eligibility trace\n",
        "  \"\"\"\n",
        "  thres = args['thres']\n",
        "  t_rise = args['t_rise']\n",
        "  t_decay = args['t_decay']\n",
        "  \n",
        "  beta = 1 # mV^-1\n",
        "  h_i = beta*(mem - thres)\n",
        "\n",
        "  post = 1 / (1 + torch.abs(h_i))**2 \n",
        "  pre_synaptic_trace = doubleExponential(spike_train, dt, t_rise, t_decay, time)\n",
        "\n",
        "  hebbian = post * pre_synaptic_trace #hebbian coincidence term\n",
        "\n",
        "  #synaptic eligibility trace\n",
        "  eligibility_trace = doubleExponential(hebbian, dt, t_rise, t_decay, time)\n",
        "\n",
        "  return elibility_trace"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BATIjT7BlD1t"
      },
      "source": [
        "### Error Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4fIjji7XOok"
      },
      "source": [
        "#@title Output Error Signal\n",
        "\n",
        "def error_signal(spike_train, desired_spike_train):\n",
        "  \"\"\"\n",
        "  Returns the error signal (time series)\n",
        "  \"\"\"\n",
        "  # output spike train\n",
        "  filter_1 = doubleExponential(spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  # desired spike train\n",
        "  filter_2 = doubleExponential(desired_spike_train, dt, t_rise,\n",
        "                               t_decay, time)\n",
        "  \n",
        "  error = filter_2 - filter_1\n",
        "  return error\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0epZHtN-DCY"
      },
      "source": [
        "def error_signal2(value_exp1, value_exp2, output, target, args):\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "  dt = args['timestep_size']\n",
        "\n",
        "  difference = target - output\n",
        "\n",
        "  z = value_exp1 + (-value_exp1/t_rise + difference)*dt\n",
        "  z_hat = value_exp2 + (-value_exp2 + value_exp1)*dt/t_decay\n",
        "\n",
        "\n",
        "  return z, z_hat\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0UfjVzQZti"
      },
      "source": [
        "#@title Feedback Signal\n",
        "\n",
        "def feedback_signal():\n",
        "  \"\"\"\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  return feedback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJ_QKUSTO0F"
      },
      "source": [
        "### Per Parameter Learning Rate\n",
        "\n",
        "Zenke & Ganguli (2018) used a per parameter learning rate. **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDZpUAxTNOb"
      },
      "source": [
        "def learning_rate():\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsyQm0lXdch"
      },
      "source": [
        "### Regularization Term\n",
        "\n",
        "Heterosynaptic regularization term to the learning rule of the hidden layers to avoid pathologically high firing rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXCD25nXg8k"
      },
      "source": [
        "def regularization_term():\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJKFm5liSxym"
      },
      "source": [
        "### Poisson Spike Trains\n",
        "\n",
        "Based on the method suggested by David Heeger [here](https://www.cns.nyu.edu/~david/handouts/poisson.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6HSjSVcSDKA"
      },
      "source": [
        "def Poisson_trains(n, lam, timesteps, dt):\n",
        "  \"\"\"\n",
        "\n",
        "  inputs:\n",
        "    n - number of poisson spike trains \n",
        "    lam - 1-D array containing mean value of poisson trains\n",
        "  Returns\n",
        "\n",
        "  \"\"\"\n",
        "  trains = torch.zeros((n, timesteps), device=device, dtype=dtype)\n",
        "  unif = torch.rand((n, timesteps), device=device, dtype=dtype)\n",
        "\n",
        "#  counter = 0\n",
        "  for i in range(n):\n",
        "    trains[unif <= lam[i]*dt] = 1\n",
        "#    counter += len(unif <= lam[i]*dt)\n",
        "#  print(\"Total No. of Spikes\", counter)\n",
        "\n",
        "  return trains"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRVvazAW4264"
      },
      "source": [
        "trains = Poisson_trains(1, 1000*np.ones(10), 1000, 1e-4)\n",
        "\n",
        "#print(trains)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoctDxVETK6F"
      },
      "source": [
        "#@title Step Function for Spikes\n",
        "def spike_fn(x, thres):\n",
        "  \"\"\"\n",
        "  Implements a heaviside function centred at the firing threshold\n",
        "  \"\"\"\n",
        "  x = x - thres\n",
        "  out = torch.zeros_like(x)\n",
        "  out[x > 0] = 1\n",
        "  return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoVT0g8hHomP"
      },
      "source": [
        "def van_rossum_loss(output, target, args):\n",
        "\n",
        "  z = 0\n",
        "  z_hat = 0\n",
        "  t_rise = args['t_rise_alpha']\n",
        "  t_decay = args['t_decay_alpha']\n",
        "  dt = args['timestep_size']\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  for i in range(len(output)):\n",
        "\n",
        "    difference = target[i] - output[i]\n",
        "    z = z + (-z/t_rise + difference)*dt\n",
        "    z_hat = z_hat + (-z_hat + z)*dt/t_decay\n",
        "    \n",
        "    loss += dt*z_hat**2 \n",
        "\n",
        "  return (1/2)*loss\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDUPS2aHRRG7"
      },
      "source": [
        "### Single Neuron Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50_ZIOtqD9ov"
      },
      "source": [
        "Property | Value|\n",
        "-----| -----|\n",
        "threshold | -50 mV\n",
        "U_rest | -60 mV\n",
        "tau_mem| 10 ms\n",
        "tau_syn| 5 ms\n",
        "tau_ref| 5 ms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NatNAnH8RP0m"
      },
      "source": [
        "nb_inputs = 100 # 100 spike trains as inputs that repeat every 500 ms\n",
        "nb_outputs = 1 \n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "nb_steps = 5000\n",
        "timestep_size = 1e-4 # 0.1 msec timesteps\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XrBRxpDxME"
      },
      "source": [
        "# LIF Neuron Model Parameters\n",
        "args = {'thres': -50,\n",
        "        'U_rest': -60,\n",
        "        'tau_mem': 1e-2,\n",
        "        'tau_syn': 5e-3,\n",
        "        'tau_ref': 5e-3,\n",
        "        't_rise': 5e-3, # the pre-synaptic double exponential kernel rise time\n",
        "        't_decay': 1e-2, # the pre-synaptic double exponential kernel decay time\n",
        "        'timestep_size': 1e-4,\n",
        "        't_rise_alpha': 5e-3,\n",
        "        't_decay_alpha': 1e-2} "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtO16h80uzyo"
      },
      "source": [
        "tau_syn = args['tau_syn']\n",
        "tau_mem = args['tau_mem']\n",
        "\n",
        "alpha = float(np.exp(-timestep_size/tau_syn))\n",
        "beta = float(np.exp(-timestep_size/tau_mem))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "6e_2mHe4Ip6j",
        "outputId": "abb70c76-fa97-4a06-b908-25ab54528d71"
      },
      "source": [
        "def weight_update(input, output, target, mem, args):\n",
        "  update = torch.zeros(nb_outputs)\n",
        "\n",
        "  for i in range(nb_outputs):\n",
        "    update[i] = error_signal(output, target)*eligibility_trace(mem, input[i]\n",
        "  update = error_signal(output, target)*\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-76c5f9cbcee3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    update = error_signal(output, target)*\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBlHb3qGdeE9"
      },
      "source": [
        "#@title Input Spike Trains\n",
        "\n",
        "spk_freq = 10 # not sure about this, but assuming it since the paper\n",
        "# uses 10 Hz frequency as the target output frequency (actually \n",
        "# 5 equidistant spikes over 500 ms)\n",
        "\n",
        "input_trains = Poisson_trains(100, spk_freq*np.ones(100),\n",
        "                              nb_steps, timestep_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmSXUKaCRXZT"
      },
      "source": [
        "#@title Target Spike Train\n",
        "## 5 equidistant spikes spread over 0.5 secs\n",
        "target = torch.zeros(nb_steps)\n",
        "target[:: nb_steps//5] = 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJP4zBk6T5F9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267042dd-3afc-4ce1-9868-ab10372b35ef"
      },
      "source": [
        "#@title Weight Initialization\n",
        "\n",
        "weight_scale = 7*(1 - beta) # copied from spytorch\n",
        "\n",
        "weights = torch.empty((nb_inputs, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(weights, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "print(\"Weight initialization done\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight initialization done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLTjfwXjaZ0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "d0b6e4ce-e097-4fc2-e99e-94c94e04e285"
      },
      "source": [
        "mem = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "syn = torch.zeros(nb_outputs, device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 1000\n",
        "thres = args['thres']\n",
        "\n",
        "loss_rec = []\n",
        "for i in range(epochs):\n",
        "\n",
        "  m = 0\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "\n",
        "  eligibility_trace_record = torch.zeros((nb_inputs, nb_steps), device=device, dtype=dtype)\n",
        " # pre_trace_rec = torch.zeros((nb_inputs, nb_inputs, nb_steps), device=device, dtype=dtype)\n",
        "  \n",
        "\n",
        "  eligibility_rec = []\n",
        "  pre_trace_rec = []\n",
        "\n",
        "  out_spks = torch.zeros(nb_steps, device=device, dtype=dtype)\n",
        "  \n",
        "  last_presynaptic_traces = [torch.zeros(nb_inputs), torch.zeros(nb_inputs)]\n",
        "  last_eligibility_traces = [torch.zeros((nb_inputs, nb_outputs)),\n",
        "                             torch.zeros((nb_inputs, nb_outputs))] # not sure about the shape here, might have to recheck\n",
        "\n",
        "  last_error_values = [torch.zeros((nb_inputs, nb_outputs)),\n",
        "                            torch.zeros((nb_inputs, nb_outputs))]\n",
        "  for t in range(nb_steps):\n",
        "\n",
        "    weighted_inp = input_trains[:, t] * weights\n",
        "\n",
        "    new_syn = alpha*syn + weighted_inp\n",
        "    new_mem = beta*mem + syn*(1 - beta)\n",
        "\n",
        "    mem_rec.append(new_mem)\n",
        "\n",
        "    out = spike_fn(new_mem, thres)\n",
        "    spk_rec.append(out)\n",
        "\n",
        "\n",
        "    # compute presynaptic traces\n",
        "    presynaptic_traces = presynaptic_trace(last_presynaptic_traces[0],\n",
        "                                           last_presynaptic_traces[1],\n",
        "                                           input_trains[:, t], args)\n",
        "    \n",
        "    last_presynaptic_traces = presynaptic_traces\n",
        "    ##pre_trace_rec.append(presynaptic_traces)\n",
        "\n",
        "    # evaluate hebbian coincidence and synaptic eligibility traces\n",
        "    h = mem - thres\n",
        "    post = 1 / (1 + torch.abs(h))**2\n",
        "\n",
        "    print(\"Presynaptic Traces Shape:\", presynaptic_traces[1].shape)\n",
        "    hebbian = post * presynaptic_traces[1]\n",
        "    print(\"Hebbian term shape:\", hebbian.shape)\n",
        "\n",
        "    print(\"Last Eligibility Trace Shape:\", last_eligibility_traces[1].shape)\n",
        "\n",
        "\n",
        "    synaptic_eligibility = eligibility_trace2(last_eligibility_traces[0],\n",
        "                                     last_eligibility_traces[1],\n",
        "                                     hebbian, args)\n",
        "\n",
        "    last_eligibility_traces = synaptic_eligibility\n",
        "\n",
        "  \n",
        "    error = error_signal2(last_error_values[0], last_error_values[1],\n",
        "                          out, target[t], args)\n",
        "    \n",
        "    last_error_values = error\n",
        "    # for minibatching weight updates in time\n",
        "    print(\"Error shape:\", error[1].shape)\n",
        "    print(\"Eligibility Shape:\", synaptic_eligibility[1].shape)\n",
        "\n",
        "    m += error[1] * synaptic_eligibility[1]\n",
        "  \n",
        "    print(\"\\n\")\n",
        "  weights += m\n",
        "  spk_rec = torch.stack(spk_rec, dim=1)\n",
        "  loss = van_rossum(spk_rec, target, args)\n",
        "  \n",
        "  loss_rec.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(loss_rec)\n",
        "    \n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presynaptic Traces Shape: torch.Size([100])\n",
            "Hebbian term shape: torch.Size([100])\n",
            "Last Eligibility Trace Shape: torch.Size([100, 1])\n",
            "Error shape: torch.Size([100, 1])\n",
            "Eligibility Shape: torch.Size([100, 1])\n",
            "\n",
            "\n",
            "Presynaptic Traces Shape: torch.Size([100])\n",
            "Hebbian term shape: torch.Size([100])\n",
            "Last Eligibility Trace Shape: torch.Size([100, 1])\n",
            "Error shape: torch.Size([100, 1])\n",
            "Eligibility Shape: torch.Size([100, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3563f0ee015d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Eligibility Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynaptic_eligibility\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msynaptic_eligibility\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [100, 1] doesn't match the broadcast shape [100, 100]"
          ]
        }
      ]
    }
  ]
}